{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise09 : ML Pipeline\n",
    "\n",
    "With AML pipeline, you can create ML workflows for such as following purposes.\n",
    "\n",
    "- You can build retraining pipeline for MLOps integration.\n",
    "- You can build batch-scoring pipeline instead of real-time scoring in \"[Exercise08 : Publish as a Web Service](./exercise08_publish_model.ipynb)\".\n",
    "\n",
    "> Note : See [here](https://docs.microsoft.com/en-us/azure/architecture/reference-architectures/ai/mlops-python) for the reference architecture integrating with CI/CD tools.\n",
    "\n",
    "ML pipeline can be invoked by the following methods. \n",
    "\n",
    "- Time-based schedule invocation\n",
    "- On-demand invocation by the published endpoint (REST)\n",
    "- Trigger-based invocation, such as, file change or other combined events (with Azure Event Grid, Azure Logic Apps, etc)\n",
    "\n",
    "In this exercise, we create a simple training pipeline, which returns model metrics in top-level (pipeline's) outputs.\n",
    "\n",
    "*back to [index](https://github.com/tsmatz/azureml-tutorial/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize MLClient\n",
    "\n",
    "Replace below's branket's string with your subscription id, resource group name, and AML workspace name.<br>\n",
    "(I note that creating ```MLClient``` will not connect to AML workspace, and the client initialization is lazy.)\n",
    "\n",
    "Using ```ClientSecretCredential()```, you would be able to involve ML pipeline in CI/CD utilities (such as, in GitHub actions) without login UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DeviceCodeCredential, TokenCachePersistenceOptions\n",
    "\n",
    "# When you run on remote\n",
    "cache_opt = TokenCachePersistenceOptions(allow_unencrypted_storage=True)\n",
    "cred = DeviceCodeCredential(cache_persistence_options=cache_opt)\n",
    "\n",
    "# # When you run on Azure ML Notebook\n",
    "# from azure.identity import DefaultAzureCredential\n",
    "# cred = DefaultAzureCredential()\n",
    "\n",
    "# Get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    credential=cred,\n",
    "    subscription_id=\"{SUBSCRIPTION ID}\",\n",
    "    resource_group_name=\"{RESOURCE GROUP NAME}\",\n",
    "    workspace_name=\"{AML WORKSPACE NAME}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create compute\n",
    "\n",
    "Create your new AML compute for running pipeline.\n",
    "\n",
    "When the pipeline is invoked, the compute will be started. When the pipeline is completed, this compute will be automatically scaled down to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code ADSC82ZC9 to authenticate.\n",
      "found existing:  mycluster01\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "\n",
    "try:\n",
    "    compute_target = ml_client.compute.get(\"mycluster01\")\n",
    "    print(\"found existing: \", compute_target.name)\n",
    "except Exception:\n",
    "    print(\"creating new.\")\n",
    "    compute_target = AmlCompute(\n",
    "        name=\"mycluster01\",\n",
    "        type=\"amlcompute\",\n",
    "        size=\"Standard_D2_v2\",\n",
    "        min_instances=0,\n",
    "        max_instances=1,\n",
    "        tier=\"Dedicated\",\n",
    "    )\n",
    "    compute_target = ml_client.begin_create_or_update(compute_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create an environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create a custom environment (with TensorFlow) to run scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 09_conda_pydata.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile 09_conda_pydata.yml\n",
    "name: project_environment\n",
    "dependencies:\n",
    "- python=3.8\n",
    "- pip:\n",
    "  - tensorflow==2.10.0\n",
    "channels:\n",
    "- anaconda\n",
    "- conda-forge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "\n",
    "myenv = Environment(\n",
    "    name=\"test-remote-cpu-env\",\n",
    "    description=\"This is example\",\n",
    "    conda_file=\"09_conda_pydata.yml\",\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\",\n",
    ")\n",
    "myenv = ml_client.environments.create_or_update(myenv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to [AML Studio UI](https://ml.azure.com/) and click \"Environments\". Next, click \"Custom environments\" tab and select the above environment.<br>\n",
    "Please wait until the environment image build status is succeeded.\n",
    "\n",
    "![Environment status](https://tsmatz.github.io/images/github/azure-ml-tensorflow-complete-sample/20221220_Environment_Status.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, I create a pipeline for model training, evaluation, and model registration.<br>\n",
    "In this pipeline, the following steps will be executed.\n",
    "\n",
    "1. The model is trained.\n",
    "2. The model accuracy is evaluated. The model metrics is set as pipeline's output.\n",
    "\n",
    "Each source code will then be saved as follows.\n",
    "\n",
    "- training script ```./pipeline_script/train.py```\n",
    "- evaluation script ```./pipeline_script/evaluate.py```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "script_folder = './pipeline_script'\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pipeline_script/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pipeline_script/train.py\n",
    "import os\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "\n",
    "# device test\n",
    "print(\"##### List of available GPU #####\")\n",
    "print(tf.config.list_physical_devices(\"GPU\"))\n",
    "\n",
    "# parse arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--data_folder\",\n",
    "    type=str,\n",
    "    default=\"./data/train\",\n",
    "    help=\"Folder path for input data\")\n",
    "parser.add_argument(\n",
    "    \"--model_folder\",\n",
    "    type=str,\n",
    "    default=\"./outputs\",  # AML experiments outputs folder\n",
    "    help=\"Folder path for model output\")\n",
    "parser.add_argument(\n",
    "    \"--learning_rate\",\n",
    "    type=float,\n",
    "    default=\"0.001\",\n",
    "    help=\"Learning Rate\")\n",
    "parser.add_argument(\n",
    "    \"--first_layer\",\n",
    "    type=int,\n",
    "    default=\"128\",\n",
    "    help=\"Neuron number for the first hidden layer\")\n",
    "parser.add_argument(\n",
    "    \"--second_layer\",\n",
    "    type=int,\n",
    "    default=\"64\",\n",
    "    help=\"Neuron number for the second hidden layer\")\n",
    "parser.add_argument(\n",
    "    \"--epochs_num\",\n",
    "    type=int,\n",
    "    default=\"6\",\n",
    "    help=\"Number of epochs\")\n",
    "FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "# build model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(FLAGS.first_layer, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(FLAGS.second_layer, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(FLAGS.learning_rate),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "# run training\n",
    "train_data = tf.data.experimental.load(FLAGS.data_folder)\n",
    "model.fit(\n",
    "    train_data.shuffle(1000).batch(128).prefetch(tf.data.AUTOTUNE),\n",
    "    epochs=FLAGS.epochs_num\n",
    ")\n",
    "\n",
    "# save model and variables\n",
    "model_path = os.path.join(FLAGS.model_folder, \"mnist_tf_model\")\n",
    "model.save(model_path)\n",
    "print(\"current working directory : \", os.getcwd())\n",
    "print(\"model folder : \", model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pipeline_script/evaluate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pipeline_script/evaluate.py\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--data_folder',\n",
    "    type=str,\n",
    "    default='./data/test',\n",
    "    help='Folder path for input data')\n",
    "parser.add_argument(\n",
    "    '--model_folder',\n",
    "    type=str,\n",
    "    default='./model',\n",
    "    help='Folder path for model base dir')\n",
    "parser.add_argument(\n",
    "    '--output_info',\n",
    "    type=str,\n",
    "    default='./output_info',\n",
    "    help='File path for model registration info')\n",
    "FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "# load data\n",
    "test_data = tf.data.experimental.load(FLAGS.data_folder)\n",
    "\n",
    "# load model\n",
    "model_folder_path = os.path.join(FLAGS.model_folder, \"mnist_tf_model\")\n",
    "loaded_model = tf.keras.models.load_model(model_folder_path)\n",
    "\n",
    "# evaluate\n",
    "results = loaded_model.evaluate(test_data.batch(128))\n",
    "print(\"Loss: {}, Accuracy: {}\".format(results[0], results[1]))\n",
    "\n",
    "# write result (metrics)\n",
    "output_info = {\n",
    "    \"accuracy\" : float(results[1]),\n",
    "    \"loss\" : float(results[0])\n",
    "}\n",
    "output_json = json.dumps(output_info)\n",
    "f = open(FLAGS.output_info,\"w\")\n",
    "f.write(output_json)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build and Run ML pipeline\n",
    "\n",
    "Now let's compose pipeline in yaml, and submit a job for the generated pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define command objects, which are all used in pipeline.\n",
    "\n",
    "> Note : In this example, I also use the registered data asset named ```mnist_data``` to mount in your compute target. Run \"[Exercise02 : Prepare Data](./exercise02_prepare_data.ipynb)\" for dataset preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command, Input, Output\n",
    "\n",
    "# 1. Create a command to train model\n",
    "train_model_command = command(\n",
    "    name=\"train_model\",\n",
    "    display_name=\"train_model\",\n",
    "    code=\"./pipeline_script\",\n",
    "    command=\"python train.py --data_folder ${{inputs.tf_dataset}}/train --model_folder ${{outputs.model_dir}}\",\n",
    "    environment=\"test-remote-cpu-env@latest\",\n",
    "    inputs={\n",
    "        \"tf_dataset\": Input(type=\"uri_folder\"),\n",
    "    },\n",
    "    outputs={\n",
    "        \"model_dir\": Output(type=\"uri_folder\"),\n",
    "    },\n",
    ")\n",
    "\n",
    "# 2. Create a command to evaluate model\n",
    "evaluate_model_command = command(\n",
    "    name=\"evaluate_model\",\n",
    "    display_name=\"evaluate_model\",\n",
    "    code=\"./pipeline_script\",\n",
    "    command=\"python evaluate.py --data_folder ${{inputs.tf_dataset}}/test --model_folder ${{inputs.model_dir}} --output_info ${{outputs.model_info}}/metrics.txt\",\n",
    "    environment=\"test-remote-cpu-env@latest\",\n",
    "    inputs={\n",
    "        \"tf_dataset\": Input(type=\"uri_folder\"),\n",
    "        \"model_dir\": Input(type=\"uri_folder\"),\n",
    "    },\n",
    "    outputs={\n",
    "        \"model_info\": Output(type=\"uri_folder\"),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next build pipeline with above commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.dsl import pipeline\n",
    "\n",
    "@pipeline(default_compute=\"mycluster01\")\n",
    "def training_pipeline(training_input):\n",
    "    train_node = train_model_command(\n",
    "        tf_dataset=training_input\n",
    "    )\n",
    "    eval_node = evaluate_model_command(\n",
    "        tf_dataset=training_input,\n",
    "        model_dir=train_node.outputs.model_dir\n",
    "    )\n",
    "    return {\"output_folder\": eval_node.outputs.model_info}\n",
    "\n",
    "pipeline_job = training_pipeline(\n",
    "    training_input = Input(\n",
    "        type=\"uri_folder\",\n",
    "        path=\"mnist_data@latest\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit a job to run this pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading pipeline_script (0.0 MBs): 100%|█████████████████████████████████| 2936/2936 [00:00<00:00, 92624.17it/s]\u001b[0m\n",
      "\u001b[39m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_job, experiment_name=\"training-pipeline01\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to [AML studio UI](https://ml.azure.com/) and see pipeline results in jobs. (See below.)\n",
    "\n",
    "![Pipeline results](https://tsmatz.github.io/images/github/azure-ml-tensorflow-complete-sample/20220225_Experiment_Pipeline.jpg)\n",
    "\n",
    "You can extract model metrics in pipeline outputs.<br>\n",
    "If it's passed in this training pipeline, you can then invoke the next stage in MLOps integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Remove Compute\n",
    "\n",
    "You don't need to remove your AML compute for saving money, because the nodes will be automatically terminated, when it's inactive.<br>\n",
    "But if you want to clean up, please run as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting compute mycluster01 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "(3m 7s)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ml_client.compute.begin_delete(\"mycluster01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
