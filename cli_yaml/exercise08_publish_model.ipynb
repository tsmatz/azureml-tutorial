{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise08 : Publish as a Web Service\n",
    "\n",
    "Finally we publish our model as a web service.\n",
    "\n",
    "Before running this code, **complete the model registration in \"[Exercise04 : Train on Remote GPU Virtual Machine](./exercise04_train_remote.ipynb)\"**.\n",
    "\n",
    "*back to [index](https://github.com/tsmatz/azureml-tutorial/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable's Setting\n",
    "\n",
    "Replace below's branket's string and set the required variables.\n",
    "\n",
    "> Note : By the following ```az configure --defaults```, you can skip setting for ```--resource-group``` and ```--workspace-name``` options in each ```az ml``` command.<br>\n",
    "> ```az configure --defaults group=$resource_group workspace=$aml_workspace```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_resource_group = \"{AML-RESOURCE-GROUP-NAME}\"\n",
    "my_workspace = \"{AML-WORSPACE-NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create entry script (.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to deploy as web service, first we generate the following scoring code.<br>\n",
    "This entry script in AML should include both ```init()``` and ```run()```.\n",
    "\n",
    "> Note : The serving compute (VM) provides [managed identity endpoint](https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/how-to-use-vm-token). (Your script can use both system-assigned identity and user-assigned identity.) Your script can then get the access permissions for Azure resources without providing secure information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "script_folder = './inference_script'\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing inference_script/score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference_script/score.py\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def init():\n",
    "    global loaded_model\n",
    "    ## model_path = azureml.core.Model.get_model_path(model_name='mnist_model_test')\n",
    "    model_path = os.path.join(\n",
    "        os.getenv(\"AZUREML_MODEL_DIR\"), \"mnist_tf_model\"\n",
    "    )\n",
    "    loaded_model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "def run(raw_data):\n",
    "    try:\n",
    "        data = json.loads(raw_data)[\"data\"]\n",
    "        pred_output = loaded_model(np.array(data))\n",
    "        pred_list = tf.math.argmax(pred_output, axis=-1).numpy().tolist()\n",
    "        return pred_list\n",
    "    except Exception as e:\n",
    "       result = str(e)\n",
    "       return 'Internal Exception : ' + result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a managed endpoint\n",
    "\n",
    "There exist **endpoint** and **deployment** in deployment topology in managed online endpoint.<br>\n",
    "You can run multiple deployments in a single endpoint, and allocate appropriate traffic for these multiple deployments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create a managed endpoint for deployment target.<br>\n",
    "I note that **```name``` should be unique and then specify arbitrary unique name**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"{UNIQUE_ENDPOINT_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the following ```UNIQUE_ENDPOINT_NAME```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 08_managed_endpoint.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile 08_managed_endpoint.yml\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineEndpoint.schema.json\n",
    "name: {UNIQUE_ENDPOINT_NAME}\n",
    "auth_mode: key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"auth_mode\": \"key\",\r\n",
      "  \"id\": \"/subscriptions/b3ae1c15-4fef-4362-8c3a-5d804cdeb18d/resourceGroups/rg-AML/providers/Microsoft.MachineLearningServices/workspaces/ws01/onlineEndpoints/my-mnist-test123\",\r\n",
      "  \"identity\": {\r\n",
      "    \"principal_id\": \"65f96799-6bf2-49ae-841a-246d2acd5bd1\",\r\n",
      "    \"tenant_id\": \"72f988bf-86f1-41af-91ab-2d7cd011db47\",\r\n",
      "    \"type\": \"system_assigned\"\r\n",
      "  },\r\n",
      "  \"kind\": \"Managed\",\r\n",
      "  \"location\": \"eastus\",\r\n",
      "  \"mirror_traffic\": {},\r\n",
      "  \"name\": \"my-mnist-test123\",\r\n",
      "  \"properties\": {\r\n",
      "    \"AzureAsyncOperationUri\": \"https://management.azure.com/subscriptions/b3ae1c15-4fef-4362-8c3a-5d804cdeb18d/providers/Microsoft.MachineLearningServices/locations/eastus/mfeOperationsStatus/oe:161509fe-37ed-4417-a7dc-90cfeccf6f44:f723d046-1e90-4f61-a23b-d2a1a29ce856?api-version=2022-02-01-preview\",\r\n",
      "    \"azureml.onlineendpointid\": \"/subscriptions/b3ae1c15-4fef-4362-8c3a-5d804cdeb18d/resourcegroups/rg-aml/providers/microsoft.machinelearningservices/workspaces/ws01/onlineendpoints/my-mnist-test123\"\r\n",
      "  },\r\n",
      "  \"provisioning_state\": \"Succeeded\",\r\n",
      "  \"public_network_access\": \"enabled\",\r\n",
      "  \"resourceGroup\": \"rg-AML\",\r\n",
      "  \"scoring_uri\": \"https://my-mnist-test123.eastus.inference.ml.azure.com/score\",\r\n",
      "  \"swagger_uri\": \"https://my-mnist-test123.eastus.inference.ml.azure.com/swagger.json\",\r\n",
      "  \"tags\": {},\r\n",
      "  \"traffic\": {}\r\n",
      "}\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!az ml online-endpoint create --file 08_managed_endpoint.yml \\\n",
    "  --resource-group $my_resource_group \\\n",
    "  --workspace-name $my_workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy as web service\n",
    "\n",
    "Next we deploy the serving code (```score.py```) as a web service in the previous endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before deployment, create conda configuration for serving environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 08_conda_env.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile 08_conda_env.yml\n",
    "name: serving_example\n",
    "dependencies:\n",
    "- python=3.8\n",
    "- pip:\n",
    "  - azureml-defaults\n",
    "  - tensorflow==2.10.0\n",
    "  - numpy\n",
    "channels:\n",
    "- anaconda\n",
    "- conda-forge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we deploy a web service with yaml configuration for deployment.\n",
    "\n",
    "When you change the model (or code) in managed endpoint, you can submit multiple deployments and transfer the traffic allocation without causing any disruption.<br>\n",
    "With the following ```--all-traffic``` option, all traffic (100% traffic) will be allocated to this single deployment.\n",
    "\n",
    "In this example, I use the trained model in Exercise04, and **run \"[Exercise04 : Train on Remote GPU Virtual Machine](./exercise04_train_remote.ipynb)\", before running this code.**\n",
    "\n",
    "Replace the following ```UNIQUE_ENDPOINT_NAME```.\n",
    "\n",
    "> Note : You can scale computes by increasing the following ```instance_count```. (You can also define auto-scale settings.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 08_managed_deployment.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile 08_managed_deployment.yml\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json\n",
    "name: my-mnist-deployment-v1\n",
    "endpoint_name: {UNIQUE_ENDPOINT_NAME}\n",
    "model: azureml:mnist_model_test@latest\n",
    "code_configuration:\n",
    "  code: ./inference_script\n",
    "  scoring_script: score.py\n",
    "environment: \n",
    "  conda_file: 08_conda_env.yml\n",
    "  image: mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\n",
    "instance_type: Standard_DS2_v2\n",
    "instance_count: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All traffic will be set to deployment my-mnist-deployment-v1 once it has been provisioned.\n",
      "If you interrupt this command or it times out while waiting for the provisioning, you can try to set all the traffic to this deployment later once its has been provisioned.\n",
      "Check: endpoint my-mnist-test123 exists\n",
      "\u001b[32mUploading inference_script (0.0 MBs): 100%|â–ˆ| 683/683 [00:00<00:00, 78500.25it/s\u001b[0m\n",
      "\u001b[39m\n",
      "\n",
      "Creating/updating online deployment my-mnist-deployment-v1 ................................................................................................................................................................................Done (15m 41s)\n",
      "{\n",
      "  \"app_insights_enabled\": false,\n",
      "  \"code_configuration\": {\n",
      "    \"code\": \"/subscriptions/b3ae1c15-4fef-4362-8c3a-5d804cdeb18d/resourceGroups/rg-AML/providers/Microsoft.MachineLearningServices/workspaces/ws01/codes/ec542dcc-ce84-4df7-8fa2-d28b986519ed/versions/1\",\n",
      "    \"scoring_script\": \"score.py\"\n",
      "  },\n",
      "  \"endpoint_name\": \"my-mnist-test123\",\n",
      "  \"environment\": \"azureml:/subscriptions/b3ae1c15-4fef-4362-8c3a-5d804cdeb18d/resourceGroups/rg-AML/providers/Microsoft.MachineLearningServices/workspaces/ws01/environments/CliV2AnonymousEnvironment/versions/2f25fc974cf6257ab1eae35d458315b3\",\n",
      "  \"environment_variables\": {},\n",
      "  \"instance_count\": 1,\n",
      "  \"instance_type\": \"Standard_DS2_v2\",\n",
      "  \"model\": \"azureml:/subscriptions/b3ae1c15-4fef-4362-8c3a-5d804cdeb18d/resourceGroups/rg-AML/providers/Microsoft.MachineLearningServices/workspaces/ws01/models/mnist_model_test/versions/1\",\n",
      "  \"name\": \"my-mnist-deployment-v1\",\n",
      "  \"properties\": {},\n",
      "  \"tags\": {},\n",
      "  \"type\": \"managed\"\n",
      "}\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!az ml online-deployment create --file 08_managed_deployment.yml \\\n",
    "  --resource-group $my_resource_group \\\n",
    "  --workspace-name $my_workspace \\\n",
    "  --all-traffic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Optional] If error has occured, you can see logs as follows.<br>\n",
    "For instance, the following log output shows Python import error in entry script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mCommand group 'ml online-deployment' is in preview and under development. Reference and support levels: https://aka.ms/CLI_refstatus\u001b[0m\n",
      "Instance status:\n",
      "SystemSetup: Succeeded\n",
      "UserContainerImagePull: Succeeded\n",
      "ModelDownload: Succeeded\n",
      "UserContainerStart: InProgress\n",
      "\n",
      "Container logs:\n",
      "2022-02-28T07:19:01,824500903+00:00 - rsyslog/run \n",
      "2022-02-28T07:19:01,827055025+00:00 - gunicorn/run \n",
      "Dynamic Python package installation is disabled.\n",
      "Starting HTTP server\n",
      "2022-02-28T07:19:01,844367081+00:00 - nginx/run \n",
      "2022-02-28T07:19:01,856197687+00:00 - iot-server/run \n",
      "EdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n",
      "2022-02-28T07:19:02,048559714+00:00 - iot-server/finish 1 0\n",
      "2022-02-28T07:19:02,050840535+00:00 - Exit code 1 is normal. Not restarting iot-server.\n",
      "Starting gunicorn 20.1.0\n",
      "Listening at: http://127.0.0.1:31311 (12)\n",
      "Using worker: sync\n",
      "worker timeout is set to 300\n",
      "Booting worker with pid: 37\n",
      "SPARK_HOME not set. Skipping PySpark Initialization.\n",
      "Initializing logger\n",
      "2022-02-28 07:19:05,419 | root | INFO | Starting up app insights client\n",
      "logging socket was found. logging is available.\n",
      "logging socket was found. logging is available.\n",
      "2022-02-28 07:19:05,419 | root | INFO | Starting up request id generator\n",
      "2022-02-28 07:19:05,419 | root | INFO | Starting up app insight hooks\n",
      "2022-02-28 07:19:05,419 | root | INFO | Invoking user's init function\n",
      "2022-02-28 07:19:05,419 | root | ERROR | User's init function failed\n",
      "2022-02-28 07:19:05,420 | root | ERROR | Encountered Exception Traceback (most recent call last):\n",
      "  File \"/var/azureml-server/aml_blueprint.py\", line 191, in register\n",
      "    main.init()\n",
      "  File \"/var/azureml-app/cli_yaml/score.py\", line 8, in init\n",
      "    model_path = os.path.join(\n",
      "NameError: name 'os' is not defined\n",
      "\n",
      "2022-02-28 07:19:05,420 | root | INFO | Waiting for logs to be sent to Application Insights before exit.\n",
      "2022-02-28 07:19:05,420 | root | INFO | Waiting 30 seconds for upload.\n",
      "\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!az ml online-deployment get-logs \\\n",
    "  --endpoint-name $endpoint_name \\\n",
    "  --name my-mnist-deployment-v1 \\\n",
    "  --resource-group $my_resource_group \\\n",
    "  --workspace-name $my_workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before submitting a deployment on cloud, you can submit and debug your deployment on local docker runtime. (See [here](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-managed-online-endpoints).)<br>\n",
    "With Visual Studio Code, you can also attach debugger on local deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your web service\n",
    "\n",
    "Let's invoke your deployed web service and check the returned results in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First see URI (address) for your deployed web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"https://my-mnist-test123.eastus.inference.ml.azure.com/score\"\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!az ml online-endpoint show \\\n",
    "  --name $endpoint_name \\\n",
    "  --query scoring_uri \\\n",
    "  --resource-group $my_resource_group \\\n",
    "  --workspace-name $my_workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract key credential for this endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"primaryKey\": \"bbH40mTXMkYMCUyibUbK9Ta5beXAdaxR\",\n",
      "  \"secondaryKey\": \"GZt56bE2XeRaHTvqRAtFG2W2HLiVtJ9R\"\n",
      "}\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!az ml online-endpoint get-credentials \\\n",
    "  --name $endpoint_name \\\n",
    "  --resource-group $my_resource_group \\\n",
    "  --workspace-name $my_workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's invoke scoring web service in Python.<br>\n",
    "(**Replace the following ```UNIQUE_ENDPOINT_NAME``` and ```API_KEY``` with yours**.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-04 09:02:22.513146: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-04 09:02:22.656867: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-04 09:02:22.656908: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-04 09:02:22.689794: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-04 09:02:23.562819: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-04 09:02:23.563007: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-04 09:02:23.563022: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted :  [7, 2, 1]\n",
      "Actual    :  [7, 2, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-04 09:02:24.294638: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-10-04 09:02:24.294678: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-04 09:02:24.294717: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (client1004): /proc/driver/nvidia/version does not exist\n",
      "2022-10-04 09:02:24.294941: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "SERVING_URI = \"https://{UNIQUE_ENDPOINT_NAME}.eastus.inference.ml.azure.com/score\"\n",
    "API_KEY = \"{API_KEY}\"\n",
    "\n",
    "# Read data by tensor\n",
    "test_data = tf.data.Dataset.load(\"./data/test\")\n",
    "\n",
    "# Generate data\n",
    "image_arr = []\n",
    "label_arr = []\n",
    "for image, label in test_data.take(3):\n",
    "    image_arr.append(image.numpy().tolist())\n",
    "    label_arr.append(label.numpy().item())\n",
    "\n",
    "# Invoke web service !\n",
    "headers = {\n",
    "    'Content-Type':'application/json',\n",
    "    'Authorization':('Bearer '+ API_KEY)\n",
    "} \n",
    "values = json.dumps(image_arr)\n",
    "input_data = \"{\\\"data\\\": \" + values + \"}\"\n",
    "http_res = requests.post(\n",
    "    SERVING_URI,\n",
    "    input_data,\n",
    "    headers = headers)\n",
    "print('Predicted : ', http_res.text)\n",
    "print('Actual    : ', label_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Delete request initiated. If you interrupt this command or it times out while waiting for deletion to complete, status can be checked using `az ml online-endpoint show -n my-mnist-test123`\n",
      "\n",
      "Deleting endpoint my-mnist-test123 \n",
      "................................................................................................................Done (9m 35s)\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!az ml online-endpoint delete \\\n",
    "  --name $endpoint_name \\\n",
    "  --resource-group $my_resource_group \\\n",
    "  --workspace-name $my_workspace \\\n",
    "  --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
