{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise09 : ML Pipeline\n",
    "\n",
    "With AML pipeline, you can create ML workflows for such as following purposes.\n",
    "\n",
    "- You can build retraining pipeline for MLOps integration.\n",
    "- You can build batch-scoring pipeline instead of real-time scoring in \"[Exercise08 : Publish as a Web Service](./exercise08_publish_model.ipynb)\".\n",
    "\n",
    "> Note : See [here](https://docs.microsoft.com/en-us/azure/architecture/reference-architectures/ai/mlops-python) for the reference architecture integrating with CI/CD tools.\n",
    "\n",
    "ML pipeline can be invoked by the following methods. \n",
    "\n",
    "- Time-based schedule invocation\n",
    "- On-demand invocation by the published endpoint (REST)\n",
    "- Trigger-based invocation, such as, file change or other combined events (with Azure Event Grid, Azure Logic Apps, etc)\n",
    "\n",
    "In this exercise, we create a simple training pipeline, which returns model metrics in top-level (pipeline's) outputs.\n",
    "\n",
    "*back to [index](https://github.com/tsmatz/azureml-tutorial/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Variable's Setting\n",
    "\n",
    "Replace below's branket's string and set the required variables.\n",
    "\n",
    "Using ```az login --service-principal```, you would be able to involve ML pipeline in CI/CD utilities (such as, in GitHub actions) without login UI.\n",
    "\n",
    "> Note : By the following ```az configure --defaults```, you can skip setting for ```--resource-group``` and ```--workspace-name``` options in each ```az ml``` command.<br>\n",
    "> ```az configure --defaults group=$resource_group workspace=$aml_workspace```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_resource_group = \"{AML-RESOURCE-GROUP-NAME}\"\n",
    "my_workspace = \"{AML-WORSPACE-NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create compute\n",
    "\n",
    "Create your new AML compute for running pipeline.\n",
    "\n",
    "When the pipeline is invoked, the compute will be started. When the pipeline is completed, this compute will be automatically scaled down to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"id\": \"/subscriptions/b3ae1c15-4fef-4362-8c3a-5d804cdeb18d/resourceGroups/AML-rg/providers/Microsoft.MachineLearningServices/workspaces/ws01/computes/mycluster01\",\r\n",
      "  \"idle_time_before_scale_down\": 120,\r\n",
      "  \"location\": \"eastus\",\r\n",
      "  \"max_instances\": 1,\r\n",
      "  \"min_instances\": 0,\r\n",
      "  \"name\": \"mycluster01\",\r\n",
      "  \"network_settings\": {},\r\n",
      "  \"provisioning_state\": \"Succeeded\",\r\n",
      "  \"resourceGroup\": \"AML-rg\",\r\n",
      "  \"size\": \"STANDARD_D2_V2\",\r\n",
      "  \"ssh_public_access_enabled\": true,\r\n",
      "  \"tier\": \"dedicated\",\r\n",
      "  \"type\": \"amlcompute\"\r\n",
      "}\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!az ml compute create --name mycluster01 \\\n",
    "  --resource-group $my_resource_group \\\n",
    "  --workspace-name $my_workspace \\\n",
    "  --type amlcompute \\\n",
    "  --min-instances 0 \\\n",
    "  --max-instances 1 \\\n",
    "  --size Standard_D2_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create an environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create a custom environment (with TensorFlow 1.15) to run scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 09_conda_pydata.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile 09_conda_pydata.yml\n",
    "name: project_environment\n",
    "dependencies:\n",
    "- python=3.6\n",
    "- pip:\n",
    "  - tensorflow==1.15\n",
    "channels:\n",
    "- anaconda\n",
    "- conda-forge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 09_env_register.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile 09_env_register.yml\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/environment.schema.json\n",
    "name: test-remote-cpu-env\n",
    "image: mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\n",
    "conda_file: 09_conda_pydata.yml\n",
    "description: This is example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"conda_file\": {\r\n",
      "    \"channels\": [\r\n",
      "      \"anaconda\",\r\n",
      "      \"conda-forge\"\r\n",
      "    ],\r\n",
      "    \"dependencies\": [\r\n",
      "      \"python=3.6\",\r\n",
      "      {\r\n",
      "        \"pip\": [\r\n",
      "          \"azureml-defaults\",\r\n",
      "          \"tensorflow==1.15\"\r\n",
      "        ]\r\n",
      "      }\r\n",
      "    ],\r\n",
      "    \"name\": \"project_environment\"\r\n",
      "  },\r\n",
      "  \"creation_context\": {\r\n",
      "    \"created_at\": \"2022-06-07T05:29:52.216059+00:00\",\r\n",
      "    \"created_by\": \"Tsuyoshi Matsuzaki\",\r\n",
      "    \"created_by_type\": \"User\",\r\n",
      "    \"last_modified_at\": \"2022-06-07T05:29:52.216059+00:00\",\r\n",
      "    \"last_modified_by\": \"Tsuyoshi Matsuzaki\",\r\n",
      "    \"last_modified_by_type\": \"User\"\r\n",
      "  },\r\n",
      "  \"description\": \"This is example\",\r\n",
      "  \"id\": \"azureml:/subscriptions/b3ae1c15-4fef-4362-8c3a-5d804cdeb18d/resourceGroups/AML-rg/providers/Microsoft.MachineLearningServices/workspaces/ws01/environments/test-remote-cpu-env/versions/1\",\r\n",
      "  \"image\": \"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\",\r\n",
      "  \"name\": \"test-remote-cpu-env\",\r\n",
      "  \"os_type\": \"linux\",\r\n",
      "  \"resourceGroup\": \"AML-rg\",\r\n",
      "  \"tags\": {},\r\n",
      "  \"version\": \"1\"\r\n",
      "}\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!az ml environment create --file 09_env_register.yml \\\n",
    "  --resource-group $my_resource_group \\\n",
    "  --workspace-name $my_workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, I create a pipeline for model training, evaluation, and model registration.<br>\n",
    "In this pipeline, the following steps will be executed.\n",
    "\n",
    "1. The model is trained.\n",
    "2. The model accuracy is evaluated. The model metrics is set as pipeline's output.\n",
    "\n",
    "Each source code will then be saved as follows.\n",
    "\n",
    "- training script ```./pipeline_script/train.py```\n",
    "- evaluation script ```./pipeline_script/evaluate.py```\n",
    "\n",
    "Model name (sub folder name in model dir) is saved in model info file (JSON text), which is passed into the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "script_folder = './pipeline_script'\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pipeline_script/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pipeline_script/train.py\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import argparse\n",
    "import math\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "FLAGS = None\n",
    "batch_size = 100\n",
    "\n",
    "#\n",
    "# Define functions for Estimator\n",
    "#\n",
    "\n",
    "def _my_input_fn(filepath, num_epochs):\n",
    "    # image - 784 (=28 x 28) elements of grey-scaled integer value [0, 1]\n",
    "    # label - digit (0, 1, ..., 9)\n",
    "    data_queue = tf.train.string_input_producer(\n",
    "        [filepath],\n",
    "        num_epochs = num_epochs) # data is repeated and it raises OutOfRange when data is over\n",
    "    data_reader = tf.TFRecordReader()\n",
    "    _, serialized_exam = data_reader.read(data_queue)\n",
    "    data_exam = tf.parse_single_example(\n",
    "        serialized_exam,\n",
    "        features={\n",
    "            'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "            'label': tf.FixedLenFeature([], tf.int64)\n",
    "        })\n",
    "    data_image = tf.decode_raw(data_exam['image_raw'], tf.uint8)\n",
    "    data_image.set_shape([784])\n",
    "    data_image = tf.cast(data_image, tf.float32) * (1. / 255)\n",
    "    data_label = tf.cast(data_exam['label'], tf.int32)\n",
    "    data_batch_image, data_batch_label = tf.train.batch(\n",
    "        [data_image, data_label],\n",
    "        batch_size=batch_size)\n",
    "    return {'inputs': data_batch_image}, data_batch_label\n",
    "\n",
    "def _get_input_fn(filepath, num_epochs):\n",
    "    return lambda: _my_input_fn(filepath, num_epochs)\n",
    "\n",
    "def _my_model_fn(features, labels, mode):\n",
    "    # with tf.device(...): # You can set device if using GPUs\n",
    "\n",
    "    # define network and inference\n",
    "    # (simple 2 fully connected hidden layer : 784->128->64->10)\n",
    "    with tf.name_scope('hidden1'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [784, FLAGS.first_layer],\n",
    "                stddev=1.0 / math.sqrt(float(784))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(\n",
    "            tf.zeros([FLAGS.first_layer]),\n",
    "            name='biases')\n",
    "        hidden1 = tf.nn.relu(tf.matmul(features['inputs'], weights) + biases)\n",
    "    with tf.name_scope('hidden2'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [FLAGS.first_layer, FLAGS.second_layer],\n",
    "                stddev=1.0 / math.sqrt(float(FLAGS.first_layer))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(\n",
    "            tf.zeros([FLAGS.second_layer]),\n",
    "            name='biases')\n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [FLAGS.second_layer, 10],\n",
    "                stddev=1.0 / math.sqrt(float(FLAGS.second_layer))),\n",
    "        name='weights')\n",
    "        biases = tf.Variable(\n",
    "            tf.zeros([10]),\n",
    "            name='biases')\n",
    "        logits = tf.matmul(hidden2, weights) + biases\n",
    " \n",
    "    # compute evaluation matrix\n",
    "    predicted_indices = tf.argmax(input=logits, axis=1)\n",
    "    if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "        label_indices = tf.cast(labels, tf.int32)\n",
    "        accuracy = tf.metrics.accuracy(label_indices, predicted_indices)\n",
    "        tf.summary.scalar('accuracy', accuracy[1]) # output to TensorBoard\n",
    " \n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(\n",
    "            labels=labels,\n",
    "            logits=logits)\n",
    " \n",
    "    # define operations\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        #global_step = tf.train.create_global_step()\n",
    "        #global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "        global_step = tf.train.get_or_create_global_step()        \n",
    "        optimizer = tf.train.GradientDescentOptimizer(\n",
    "            learning_rate=FLAGS.learning_rate)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=global_step)\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode,\n",
    "            loss=loss,\n",
    "            train_op=train_op)\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        eval_metric_ops = {\n",
    "            'accuracy': accuracy\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode,\n",
    "            loss=loss,\n",
    "            eval_metric_ops=eval_metric_ops)\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        probabilities = tf.nn.softmax(logits, name='softmax_tensor')\n",
    "        predictions = {\n",
    "            'classes': predicted_indices,\n",
    "            'probabilities': probabilities\n",
    "        }\n",
    "        export_outputs = {\n",
    "            'prediction': tf.estimator.export.PredictOutput(predictions)\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode,\n",
    "            predictions=predictions,\n",
    "            export_outputs=export_outputs)\n",
    "\n",
    "def _my_serving_input_fn():\n",
    "    inputs = {'inputs': tf.placeholder(tf.float32, [None, 784])}\n",
    "    return tf.estimator.export.ServingInputReceiver(inputs, inputs)\n",
    "\n",
    "_my_evaluation_input_fn = (tf.estimator.experimental.build_raw_supervised_input_receiver_fn(\n",
    "    {'inputs': tf.placeholder(dtype=tf.float32, shape=[None, 784])},\n",
    "    tf.placeholder(dtype=tf.int32, shape=[None])))\n",
    "\n",
    "#\n",
    "# Main\n",
    "#\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--data_folder',\n",
    "    type=str,\n",
    "    default='./data',\n",
    "    help='Folder path for input data')\n",
    "parser.add_argument(\n",
    "    '--chkpoint_folder',\n",
    "    type=str,\n",
    "    default='./logs',  # AML experiments logs folder\n",
    "    help='Folder path for checkpoint files')\n",
    "parser.add_argument(\n",
    "    '--model_temp',\n",
    "    type=str,\n",
    "    default='./outputs',\n",
    "    help='Folder path for model temporary output')\n",
    "parser.add_argument(\n",
    "    '--model_folder',\n",
    "    type=str,\n",
    "    help='Folder path for model output')\n",
    "parser.add_argument(\n",
    "    '--learning_rate',\n",
    "    type=float,\n",
    "    default='0.07',\n",
    "    help='Learning Rate')\n",
    "parser.add_argument(\n",
    "    '--first_layer',\n",
    "    type=int,\n",
    "    default='128',\n",
    "    help='Neuron number for the first hidden layer')\n",
    "parser.add_argument(\n",
    "    '--second_layer',\n",
    "    type=int,\n",
    "    default='64',\n",
    "    help='Neuron number for the second hidden layer')\n",
    "FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "# Clean checkpoint and model folder if exists\n",
    "if os.path.exists(FLAGS.chkpoint_folder) :\n",
    "    for file_name in os.listdir(FLAGS.chkpoint_folder):\n",
    "        file_path = os.path.join(FLAGS.chkpoint_folder, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "        elif os.path.isdir(file_path):\n",
    "            shutil.rmtree(file_path)\n",
    "if os.path.exists(FLAGS.model_temp) :\n",
    "    for file_name in os.listdir(FLAGS.model_temp):\n",
    "        file_path = os.path.join(FLAGS.model_temp, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "        elif os.path.isdir(file_path):\n",
    "            shutil.rmtree(file_path)\n",
    "\n",
    "# Read TF_CONFIG\n",
    "run_config = tf.estimator.RunConfig()\n",
    "\n",
    "# Create Estimator\n",
    "mnist_fullyconnected_classifier = tf.estimator.Estimator(\n",
    "    model_fn=_my_model_fn,\n",
    "    model_dir=FLAGS.chkpoint_folder,\n",
    "    config=run_config)\n",
    "train_spec = tf.estimator.TrainSpec(\n",
    "    input_fn=_get_input_fn(os.path.join(FLAGS.data_folder, 'train.tfrecords'), 2),\n",
    "    max_steps=60000 * 2 / batch_size)\n",
    "eval_spec = tf.estimator.EvalSpec(\n",
    "    input_fn=_get_input_fn(os.path.join(FLAGS.data_folder, 'test.tfrecords'), 1),\n",
    "    steps=10000 * 1 / batch_size,\n",
    "    start_delay_secs=0)\n",
    "\n",
    "# Run training !\n",
    "tf.estimator.train_and_evaluate(\n",
    "    mnist_fullyconnected_classifier,\n",
    "    train_spec,\n",
    "    eval_spec\n",
    ")\n",
    "\n",
    "# Save model and parameters\n",
    "model_folder = mnist_fullyconnected_classifier.experimental_export_all_saved_models(\n",
    "    export_dir_base=FLAGS.model_temp,\n",
    "    input_receiver_fn_map={\n",
    "        tf.estimator.ModeKeys.EVAL: _my_evaluation_input_fn,\n",
    "        tf.estimator.ModeKeys.PREDICT: _my_serving_input_fn\n",
    "    })\n",
    "print('current working directory is ', os.getcwd())\n",
    "\n",
    "# Copy model to model_folder\n",
    "model_folder_path = model_folder.decode(\"utf-8\")\n",
    "model_folder_name = os.path.basename(model_folder_path)\n",
    "dest_path = os.path.join(FLAGS.model_folder, \"generated_model\")\n",
    "shutil.move(model_folder_path, dest_path)\n",
    "print('model is saved ', dest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pipeline_script/evaluate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pipeline_script/evaluate.py\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "FLAGS = None\n",
    "batch_size = 100\n",
    "\n",
    "def _my_input_fn(filepath, num_epochs):\n",
    "    # image - 784 (=28 x 28) elements of grey-scaled integer value [0, 1]\n",
    "    # label - digit (0, 1, ..., 9)\n",
    "    data_queue = tf.train.string_input_producer(\n",
    "        [filepath],\n",
    "        num_epochs = num_epochs) # data is repeated and it raises OutOfRange when data is over\n",
    "    data_reader = tf.TFRecordReader()\n",
    "    _, serialized_exam = data_reader.read(data_queue)\n",
    "    data_exam = tf.parse_single_example(\n",
    "        serialized_exam,\n",
    "        features={\n",
    "            'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "            'label': tf.FixedLenFeature([], tf.int64)\n",
    "        })\n",
    "    data_image = tf.decode_raw(data_exam['image_raw'], tf.uint8)\n",
    "    data_image.set_shape([784])\n",
    "    data_image = tf.cast(data_image, tf.float32) * (1. / 255)\n",
    "    data_label = tf.cast(data_exam['label'], tf.int32)\n",
    "    data_batch_image, data_batch_label = tf.train.batch(\n",
    "        [data_image, data_label],\n",
    "        batch_size=batch_size)\n",
    "    return {'inputs': data_batch_image}, data_batch_label\n",
    "\n",
    "def _get_input_fn(filepath, num_epochs):\n",
    "    return lambda: _my_input_fn(filepath, num_epochs)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--data_folder',\n",
    "    type=str,\n",
    "    default='./data',\n",
    "    help='Folder path for input data')\n",
    "parser.add_argument(\n",
    "    '--model_folder',\n",
    "    type=str,\n",
    "    default='./model',\n",
    "    help='Folder path for model base dir')\n",
    "parser.add_argument(\n",
    "    '--output_info',\n",
    "    type=str,\n",
    "    default='./output_info',\n",
    "    help='File path for model registration info')\n",
    "FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "# Load model\n",
    "model_folder_path = os.path.join(FLAGS.model_folder, \"generated_model\")\n",
    "est = tf.contrib.estimator.SavedModelEstimator(model_folder_path)\n",
    "\n",
    "# Evaluate and output !\n",
    "eval_results = est.evaluate(\n",
    "    input_fn=_get_input_fn(os.path.join(FLAGS.data_folder, 'test.tfrecords'), 1),\n",
    "    steps=10000 * 1 / batch_size)\n",
    "print(\n",
    "    \"Accuracy: {}, Loss: {}\".format(\n",
    "        eval_results['metrics/accuracy'], eval_results['loss']\n",
    "    )\n",
    ")\n",
    "output_info = {\n",
    "    'accuracy' : float(eval_results['metrics/accuracy']),\n",
    "    'loss' : float(eval_results['loss'])\n",
    "}\n",
    "output_json = json.dumps(output_info)\n",
    "f = open(FLAGS.output_info,\"w\")\n",
    "f.write(output_json)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build and Run ML pipeline\n",
    "\n",
    "Now let's compose pipeline in yaml, and submit a job for the generated pipeline.\n",
    "\n",
    "> Note : In this example, I also use the registered data asset  (train.tfrecords, test.tfrecords) named ```mnist_tfrecords_data``` to mount in your compute target. Run \"[Exercise02 : Prepare Data](./exercise02_prepare_data.ipynb)\" for dataset preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 09_training_pipeline_job.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile 09_training_pipeline_job.yml\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/pipelineJob.schema.json\n",
    "type: pipeline\n",
    "display_name: training-pipeline01\n",
    "experiment_name: training-pipeline01\n",
    "compute: azureml:mycluster01\n",
    "inputs:\n",
    "  mnist_tf:\n",
    "    type: uri_folder\n",
    "    path: azureml:mnist_tfrecords_data@latest\n",
    "outputs:\n",
    "  model_metrics:\n",
    "jobs:\n",
    "  train_model:\n",
    "    name: train_model\n",
    "    display_name: train_model\n",
    "    command: >-\n",
    "      python train.py\n",
    "      --data_folder ${{inputs.tf_dataset}}\n",
    "      --model_folder ${{outputs.model_dir}}\n",
    "    code: pipeline_script\n",
    "    environment: azureml:test-remote-cpu-env@latest\n",
    "    inputs:\n",
    "      tf_dataset: ${{parent.inputs.mnist_tf}}\n",
    "    outputs:\n",
    "      model_dir:\n",
    "  evaluate_model:\n",
    "    name: evaluate_model\n",
    "    display_name: evaluate_model\n",
    "    command: >-\n",
    "      python evaluate.py\n",
    "      --data_folder ${{inputs.tf_dataset}}\n",
    "      --model_folder ${{inputs.model_dir}}\n",
    "      --output_info ${{outputs.model_info}}/metrics.txt\n",
    "    code: pipeline_script\n",
    "    environment: azureml:test-remote-cpu-env@latest\n",
    "    inputs:\n",
    "      tf_dataset: ${{parent.inputs.mnist_tf}}\n",
    "      model_dir: ${{parent.jobs.train_model.outputs.model_dir}}\n",
    "    outputs:\n",
    "      model_info: ${{parent.outputs.model_metrics}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit a job to run this pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading pipeline_script (0.01 MBs): 100%|█| 10154/10154 [00:00<00:00, 358227.6\u001b[0m\n",
      "\u001b[39m\n",
      "\n",
      "{\n",
      "  \"compute\": \"azureml:mycluster01\",\n",
      "  \"creation_context\": {\n",
      "    \"created_at\": \"2022-06-07T07:10:55.905612+00:00\",\n",
      "    \"created_by\": \"Tsuyoshi Matsuzaki\",\n",
      "    \"created_by_type\": \"User\"\n",
      "  },\n",
      "  \"display_name\": \"training-pipeline01\",\n",
      "  \"experiment_name\": \"training-pipeline01\",\n",
      "  \"id\": \"azureml:/subscriptions/b3ae1c15-4fef-4362-8c3a-5d804cdeb18d/resourceGroups/AML-rg/providers/Microsoft.MachineLearningServices/workspaces/ws01/jobs/loyal_grass_ln86f4gx22\",\n",
      "  \"inputs\": {\n",
      "    \"mnist_tf\": {\n",
      "      \"mode\": \"ro_mount\",\n",
      "      \"path\": \"azureml:/subscriptions/b3ae1c15-4fef-4362-8c3a-5d804cdeb18d/resourceGroups/AML-rg/providers/Microsoft.MachineLearningServices/workspaces/ws01/data/mnist_tfrecords_data/versions/1\",\n",
      "      \"type\": \"uri_folder\"\n",
      "    }\n",
      "  },\n",
      "  \"jobs\": {\n",
      "    \"evaluate_model\": {\n",
      "      \"$schema\": \"{}\",\n",
      "      \"code\": \"{}\",\n",
      "      \"command\": \"{}\",\n",
      "      \"component\": \"azureml:ce483600-4a0f-8caa-e0c6-583b996b27b0:1\",\n",
      "      \"environment_variables\": {},\n",
      "      \"inputs\": {\n",
      "        \"model_dir\": {\n",
      "          \"path\": \"${{parent.jobs.train_model.outputs.model_dir}}\"\n",
      "        },\n",
      "        \"tf_dataset\": {\n",
      "          \"path\": \"${{parent.inputs.mnist_tf}}\"\n",
      "        }\n",
      "      },\n",
      "      \"outputs\": {\n",
      "        \"model_info\": \"${{parent.outputs.model_metrics}}\"\n",
      "      },\n",
      "      \"type\": \"command\"\n",
      "    },\n",
      "    \"train_model\": {\n",
      "      \"$schema\": \"{}\",\n",
      "      \"code\": \"{}\",\n",
      "      \"command\": \"{}\",\n",
      "      \"component\": \"azureml:bb0a7b31-e29f-4e12-6d59-617e1b98f1f2:1\",\n",
      "      \"environment_variables\": {},\n",
      "      \"inputs\": {\n",
      "        \"tf_dataset\": {\n",
      "          \"path\": \"${{parent.inputs.mnist_tf}}\"\n",
      "        }\n",
      "      },\n",
      "      \"outputs\": {},\n",
      "      \"type\": \"command\"\n",
      "    }\n",
      "  },\n",
      "  \"name\": \"loyal_grass_ln86f4gx22\",\n",
      "  \"outputs\": {\n",
      "    \"model_metrics\": {\n",
      "      \"mode\": \"rw_mount\",\n",
      "      \"type\": \"uri_folder\"\n",
      "    }\n",
      "  },\n",
      "  \"properties\": {\n",
      "    \"azureml.continue_on_step_failure\": \"False\",\n",
      "    \"azureml.parameters\": \"{}\",\n",
      "    \"azureml.pipelineComponent\": \"pipelinerun\",\n",
      "    \"azureml.runsource\": \"azureml.PipelineRun\",\n",
      "    \"runSource\": \"MFE\",\n",
      "    \"runType\": \"HTTP\"\n",
      "  },\n",
      "  \"resourceGroup\": \"AML-rg\",\n",
      "  \"services\": {\n",
      "    \"Studio\": {\n",
      "      \"endpoint\": \"https://ml.azure.com/runs/loyal_grass_ln86f4gx22?wsid=/subscriptions/b3ae1c15-4fef-4362-8c3a-5d804cdeb18d/resourcegroups/AML-rg/workspaces/ws01&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\",\n",
      "      \"job_service_type\": \"Studio\"\n",
      "    },\n",
      "    \"Tracking\": {\n",
      "      \"endpoint\": \"azureml://eastus.api.azureml.ms/mlflow/v1.0/subscriptions/b3ae1c15-4fef-4362-8c3a-5d804cdeb18d/resourceGroups/AML-rg/providers/Microsoft.MachineLearningServices/workspaces/ws01?\",\n",
      "      \"job_service_type\": \"Tracking\"\n",
      "    }\n",
      "  },\n",
      "  \"settings\": {},\n",
      "  \"status\": \"Preparing\",\n",
      "  \"tags\": {},\n",
      "  \"type\": \"pipeline\"\n",
      "}\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!az ml job create --file 09_training_pipeline_job.yml \\\n",
    "  --resource-group $my_resource_group \\\n",
    "  --workspace-name $my_workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to [AML studio UI](https://ml.azure.com/) and see pipeline results in jobs. (See below.)\n",
    "\n",
    "![Pipeline results](https://tsmatz.github.io/images/github/azure-ml-tensorflow-complete-sample/20220225_Experiment_Pipeline.jpg)\n",
    "\n",
    "You can extract model metrics in pipeline outputs.<br>\n",
    "If it's passed in this training pipeline, you can then invoke the next stage in MLOps integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Remove Compute\n",
    "\n",
    "You don't need to remove your AML compute for saving money, because the nodes will be automatically terminated, when it's inactive.<br>\n",
    "But if you want to clean up, please run as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting compute mycluster01 \n",
      ".................................................Done.\n",
      "(4m 8s)\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!az ml compute delete --name mycluster01 \\\n",
    "  --resource-group $my_resource_group \\\n",
    "  --workspace-name $my_workspace \\\n",
    "  --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
