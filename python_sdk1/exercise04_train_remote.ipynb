{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise04 : Train on Remote GPU Virtual Machine\n",
    "\n",
    "Now we run our previous sample (see \"[Exercise03 : Just Train in Your Working Machine](./exercise03_train_simple.ipynb)\") on remote virtual machine with GPU utilized.<br>\n",
    "Here we use remote virtual machine and conda virtual environment, but you can also use Batch AI pool sharing in your team, or run on your favorite docker images.\n",
    "\n",
    "*back to [index](https://github.com/tsmatz/azureml-tutorial/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your training script as file (train.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create ```scirpt``` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "script_folder = './script'\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adding the following ```%%writefile``` at the beginning of the source code in \"[Exercise03 : Just Train in Your Working Machine](./exercise03_train_simple.ipynb)\", this source code is saved as ```./script/train.py```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing script/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile script/train.py\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import argparse\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "FLAGS = None\n",
    "batch_size = 100\n",
    "\n",
    "#\n",
    "# define functions for Estimator\n",
    "#\n",
    "\n",
    "def _my_input_fn(filepath, num_epochs):\n",
    "    # image - 784 (=28 x 28) elements of grey-scaled integer value [0, 1]\n",
    "    # label - digit (0, 1, ..., 9)\n",
    "    data_queue = tf.train.string_input_producer(\n",
    "        [filepath],\n",
    "        num_epochs = num_epochs) # data is repeated and it raises OutOfRange when data is over\n",
    "    data_reader = tf.TFRecordReader()\n",
    "    _, serialized_exam = data_reader.read(data_queue)\n",
    "    data_exam = tf.parse_single_example(\n",
    "        serialized_exam,\n",
    "        features={\n",
    "            'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "            'label': tf.FixedLenFeature([], tf.int64)\n",
    "        })\n",
    "    data_image = tf.decode_raw(data_exam['image_raw'], tf.uint8)\n",
    "    data_image.set_shape([784])\n",
    "    data_image = tf.cast(data_image, tf.float32) * (1. / 255)\n",
    "    data_label = tf.cast(data_exam['label'], tf.int32)\n",
    "    data_batch_image, data_batch_label = tf.train.batch(\n",
    "        [data_image, data_label],\n",
    "        batch_size=batch_size)\n",
    "    return {'inputs': data_batch_image}, data_batch_label\n",
    "\n",
    "def _get_input_fn(filepath, num_epochs):\n",
    "    return lambda: _my_input_fn(filepath, num_epochs)\n",
    "\n",
    "def _my_model_fn(features, labels, mode):\n",
    "    # with tf.device(...): # You can set device if using GPUs\n",
    "\n",
    "    # define network and inference\n",
    "    # (simple 2 fully connected hidden layer : 784->128->64->10)\n",
    "    with tf.name_scope('hidden1'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [784, FLAGS.first_layer],\n",
    "                stddev=1.0 / math.sqrt(float(784))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(\n",
    "            tf.zeros([FLAGS.first_layer]),\n",
    "            name='biases')\n",
    "        hidden1 = tf.nn.relu(tf.matmul(features['inputs'], weights) + biases)\n",
    "    with tf.name_scope('hidden2'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [FLAGS.first_layer, FLAGS.second_layer],\n",
    "                stddev=1.0 / math.sqrt(float(FLAGS.first_layer))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(\n",
    "            tf.zeros([FLAGS.second_layer]),\n",
    "            name='biases')\n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [FLAGS.second_layer, 10],\n",
    "                stddev=1.0 / math.sqrt(float(FLAGS.second_layer))),\n",
    "        name='weights')\n",
    "        biases = tf.Variable(\n",
    "            tf.zeros([10]),\n",
    "            name='biases')\n",
    "        logits = tf.matmul(hidden2, weights) + biases\n",
    " \n",
    "    # compute evaluation matrix\n",
    "    predicted_indices = tf.argmax(input=logits, axis=1)\n",
    "    if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "        label_indices = tf.cast(labels, tf.int32)\n",
    "        accuracy = tf.metrics.accuracy(label_indices, predicted_indices)\n",
    "        tf.summary.scalar('accuracy', accuracy[1]) # output to TensorBoard\n",
    " \n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(\n",
    "            labels=labels,\n",
    "            logits=logits)\n",
    " \n",
    "    # define operations\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        #global_step = tf.train.create_global_step()\n",
    "        #global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "        global_step = tf.train.get_or_create_global_step()        \n",
    "        optimizer = tf.train.GradientDescentOptimizer(\n",
    "            learning_rate=FLAGS.learning_rate)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=global_step)\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode,\n",
    "            loss=loss,\n",
    "            train_op=train_op)\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        eval_metric_ops = {\n",
    "            'accuracy': accuracy\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode,\n",
    "            loss=loss,\n",
    "            eval_metric_ops=eval_metric_ops)\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        probabilities = tf.nn.softmax(logits, name='softmax_tensor')\n",
    "        predictions = {\n",
    "            'classes': predicted_indices,\n",
    "            'probabilities': probabilities\n",
    "        }\n",
    "        export_outputs = {\n",
    "            'prediction': tf.estimator.export.PredictOutput(predictions)\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode,\n",
    "            predictions=predictions,\n",
    "            export_outputs=export_outputs)\n",
    "\n",
    "def _my_serving_input_fn():\n",
    "    inputs = {'inputs': tf.placeholder(tf.float32, [None, 784])}\n",
    "    return tf.estimator.export.ServingInputReceiver(inputs, inputs)\n",
    "\n",
    "#\n",
    "# Main\n",
    "#\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--data_folder',\n",
    "    type=str,\n",
    "    default='./data',\n",
    "    help='Folder path for input data')\n",
    "parser.add_argument(\n",
    "    '--chkpoint_folder',\n",
    "    type=str,\n",
    "    default='./logs',  # AML experiments logs folder\n",
    "    help='Folder path for checkpoint files')\n",
    "parser.add_argument(\n",
    "    '--model_folder',\n",
    "    type=str,\n",
    "    default='./outputs',  # AML experiments outputs folder\n",
    "    help='Folder path for model output')\n",
    "parser.add_argument(\n",
    "    '--learning_rate',\n",
    "    type=float,\n",
    "    default='0.07',\n",
    "    help='Learning Rate')\n",
    "parser.add_argument(\n",
    "    '--first_layer',\n",
    "    type=int,\n",
    "    default='128',\n",
    "    help='Neuron number for the first hidden layer')\n",
    "parser.add_argument(\n",
    "    '--second_layer',\n",
    "    type=int,\n",
    "    default='64',\n",
    "    help='Neuron number for the second hidden layer')\n",
    "FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "# clean checkpoint and model folder if exists\n",
    "if os.path.exists(FLAGS.chkpoint_folder) :\n",
    "    for file_name in os.listdir(FLAGS.chkpoint_folder):\n",
    "        file_path = os.path.join(FLAGS.chkpoint_folder, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "        elif os.path.isdir(file_path):\n",
    "            shutil.rmtree(file_path)\n",
    "if os.path.exists(FLAGS.model_folder) :\n",
    "    for file_name in os.listdir(FLAGS.model_folder):\n",
    "        file_path = os.path.join(FLAGS.model_folder, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "        elif os.path.isdir(file_path):\n",
    "            shutil.rmtree(file_path)\n",
    "\n",
    "# read TF_CONFIG\n",
    "run_config = tf.estimator.RunConfig()\n",
    "\n",
    "# create Estimator\n",
    "mnist_fullyconnected_classifier = tf.estimator.Estimator(\n",
    "    model_fn=_my_model_fn,\n",
    "    model_dir=FLAGS.chkpoint_folder,\n",
    "    config=run_config)\n",
    "train_spec = tf.estimator.TrainSpec(\n",
    "    input_fn=_get_input_fn(os.path.join(FLAGS.data_folder, 'train.tfrecords'), 2),\n",
    "    max_steps=60000 * 2 / batch_size)\n",
    "eval_spec = tf.estimator.EvalSpec(\n",
    "    input_fn=_get_input_fn(os.path.join(FLAGS.data_folder, 'test.tfrecords'), 1),\n",
    "    steps=10000 * 1 / batch_size,\n",
    "    start_delay_secs=0)\n",
    "\n",
    "# run !\n",
    "tf.estimator.train_and_evaluate(\n",
    "    mnist_fullyconnected_classifier,\n",
    "    train_spec,\n",
    "    eval_spec\n",
    ")\n",
    "\n",
    "# save model and variables\n",
    "model_dir = mnist_fullyconnected_classifier.export_savedmodel(\n",
    "    export_dir_base = FLAGS.model_folder,\n",
    "    serving_input_receiver_fn = _my_serving_input_fn)\n",
    "print('current working directory is ', os.getcwd())\n",
    "print('model is saved ', model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on remote VM\n",
    "\n",
    "Now let's start to integrate with AML and automate training on remote virtual machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Get workspace setting\n",
    "\n",
    "Before starting, you must read your configuration settings. (See \"[Exercise01 : Prepare Config Settings](./exercise01_prepare_config.ipynb)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "import azureml.core\n",
    "\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 : Create new remote virtual machine\n",
    "\n",
    "Create your new reomte virtual machine with GPU.<br>\n",
    "Before starting, **please check as follows**.\n",
    "\n",
    "- You should have quota for ML GPU VM in your Azure subscription. If you don't have, please request quota in Azure Portal.\n",
    "- Please fill the following ```vm_size``` and ```location``` for GPU cluster which you can use.\n",
    "\n",
    "**If you don't have any quota for GPU, please change VM size (such as, Standard_D2_v2).**\n",
    "\n",
    "> Note : It's better to use the same location for AML workspace, since data in AML workspace will be mounted on this virtual machine.\n",
    "\n",
    "By enabling auto-scaling (from 0 to 1), the node will be terminated if it's inactive. (You can save money.)    \n",
    "If VM already exists, this script will get the existing one.\n",
    "\n",
    "> Note : You can also attach an existing virtual machine (bring your own compute resource) as a compute target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating new.\n",
      "InProgress......\n",
      "SucceededProvisioning operation finished, operation \"Succeeded\"\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name='myvm01')\n",
    "    print('found existing:', compute_target.name)\n",
    "except ComputeTargetException:\n",
    "    print('creating new.')\n",
    "    compute_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size='Standard_NC4as_T4_v3', # change such as Standard_NC6 or Standard_D2_v2 if needed\n",
    "        min_nodes=0,\n",
    "        max_nodes=1,\n",
    "        location=\"eastus\")\n",
    "    compute_target = ComputeTarget.create(ws, 'myvm01', compute_config)\n",
    "    compute_target.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 : Get dataset reference for files\n",
    "\n",
    "You can use registered dataset (train.tfrecords, test.tfrecords) to mount in your compute target.    \n",
    "See \"[Exercise02 : Prepare Data](./exercise02_prepare_data.ipynb)\" for data preparation.\n",
    "\n",
    "> Note : Dataset registration is not mandatory. (You can mount any data (as dataset) in AML datastore.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "dataset = Dataset.get_by_name(ws, 'mnist_tfrecords_dataset', version='latest')\n",
    "\n",
    "# # For using unregistered data, see below\n",
    "# from azureml.core import Datastore\n",
    "# from azureml.core import Dataset\n",
    "# ds = ws.get_default_datastore()\n",
    "# ds_paths = [(ds, 'tfdata/')]\n",
    "# dataset = Dataset.File.from_files(path = ds_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 : Create environment\n",
    "\n",
    "Here we create a new docker environments for running scripts. In the first time, it will generate our own conatiner image as following settings. (It will then take a long time for completing experiment.)\n",
    "However, you can speed up by reusing the generated environment in the next run, once you have registered the generated environment.\n",
    "\n",
    "In this example, we create our own environment manually, but **you can also use existing environments (called, curated environments) for a variety of purposes**. (In Exercise 05, we will use a curated environment, which includes TensorFlow 1.x.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import DEFAULT_GPU_IMAGE\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.environment import Environment\n",
    "\n",
    "# create environment\n",
    "env = Environment('test-remote-gpu-env')\n",
    "env.python.conda_dependencies = CondaDependencies.create(\n",
    "    python_version=\"3.6\",\n",
    "    conda_packages=['tensorflow-gpu==1.15'])\n",
    "env.docker.base_image = DEFAULT_GPU_IMAGE\n",
    "\n",
    "# register environment to re-use later\n",
    "env.register(workspace=ws)\n",
    "## # speed up by using the existing environment\n",
    "## env = Environment.get(ws, name='test-remote-gpu-env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 : Run script and wait for completion\n",
    "\n",
    "> Note : It will take a long time (over 30 minutes) for the first time run, because it'll pull base image, generate new image (custom environment), start nodes in cluster, and run scripts.<br>\n",
    "> By using built-in ```AzureML-TensorFlow-1.13-GPU``` environment, it will speed up. (See Exercise 05 for using AML built-in environments, called curated environments.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: tf_remote_experiment_1630385291_61b6af45\n",
      "Web View: https://ml.azure.com/runs/tf_remote_experiment_1630385291_61b6af45?wsid=/subscriptions/b3ae1c15-4fef-4362-8c3a-5d804cdeb18d/resourcegroups/TESTML-rg/workspaces/ws01&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "\n",
      "Streaming azureml-logs/55_azureml-execution-tvmps_068e554c6f1a43ff964b2ce12414d42529985df21a7f868dc4c015ca790b955e_d.txt\n",
      "========================================================================================================================\n",
      "\n",
      "2021-08-31T04:52:35Z Running following command: /bin/bash -c sudo blobfuse /mnt/batch/tasks/shared/LS_root/jobs/ws01/azureml/tf_remote_experiment_1630385291_61b6af45/mounts/workspaceblobstore --tmp-path=/mnt/batch/tasks/shared/LS_root/jobs/ws01/azureml/tf_remote_experiment_1630385291_61b6af45/caches/workspaceblobstore -o ro --file-cache-timeout-in-seconds=1000000 --cache-size-mb=692676 -o nonempty -o allow_other --config-file=/mnt/batch/tasks/shared/LS_root/jobs/ws01/azureml/tf_remote_experiment_1630385291_61b6af45/configs/workspaceblobstore.cfg --log-level=LOG_WARNING\n",
      "2021-08-31T04:52:35Z Successfully mounted a/an Blobfuse File System at /mnt/batch/tasks/shared/LS_root/jobs/ws01/azureml/tf_remote_experiment_1630385291_61b6af45/mounts/workspaceblobstore\n",
      "2021-08-31T04:52:35Z Failed to start nvidia-fabricmanager due to exit status 5 with output Failed to start nvidia-fabricmanager.service: Unit nvidia-fabricmanager.service not found.\n",
      ". Please ignore this if the GPUs don't utilize NVIDIA® NVLink® switches.\n",
      "2021-08-31T04:52:35Z Starting output-watcher...\n",
      "2021-08-31T04:52:35Z IsDedicatedCompute == True, won't poll for Low Pri Preemption\n",
      "2021-08-31T04:52:36Z Executing 'Copy ACR Details file' on 10.0.0.5\n",
      "2021-08-31T04:52:36Z Copy ACR Details file succeeded on 10.0.0.5. Output: \n",
      ">>>   \n",
      ">>>   \n",
      "Login Succeeded\n",
      "Using default tag: latest\n",
      "latest: Pulling from azureml/azureml_1e5b59c0734bdc528077f509e1d397fe\n",
      "92473f7ef455: Pulling fs layer\n",
      "fb52bde70123: Pulling fs layer\n",
      "64788f86be3f: Pulling fs layer\n",
      "33f6d5f2e001: Pulling fs layer\n",
      "eeb715f1b6ae: Pulling fs layer\n",
      "fe519cf36537: Pulling fs layer\n",
      "58ff99196c15: Pulling fs layer\n",
      "9b13f06a8eff: Pulling fs layer\n",
      "2d4e93adbf58: Pulling fs layer\n",
      "6ee7c3767844: Pulling fs layer\n",
      "62cfc3ccb8ab: Pulling fs layer\n",
      "4a7af9d757ee: Pulling fs layer\n",
      "886c27cf0865: Pulling fs layer\n",
      "7c9062f12448: Pulling fs layer\n",
      "7268e886f68a: Pulling fs layer\n",
      "e1fdaab561c7: Pulling fs layer\n",
      "eeb715f1b6ae: Waiting\n",
      "ccb2816215bd: Pulling fs layer\n",
      "9b13f06a8eff: Waiting\n",
      "55d70b17f345: Pulling fs layer\n",
      "fe519cf36537: Waiting\n",
      "6ee7c3767844: Waiting\n",
      "58ff99196c15: Waiting\n",
      "62cfc3ccb8ab: Waiting\n",
      "4a7af9d757ee: Waiting\n",
      "886c27cf0865: Waiting\n",
      "7c9062f12448: Waiting\n",
      "2d4e93adbf58: Waiting\n",
      "7268e886f68a: Waiting\n",
      "55d70b17f345: Waiting\n",
      "e1fdaab561c7: Waiting\n",
      "ccb2816215bd: Waiting\n",
      "33f6d5f2e001: Waiting\n",
      "\n",
      "Streaming azureml-logs/65_job_prep-tvmps_068e554c6f1a43ff964b2ce12414d42529985df21a7f868dc4c015ca790b955e_d.txt\n",
      "===============================================================================================================\n",
      "\n",
      "[2021-08-31T04:53:00.848204] Entering job preparation.\n",
      "[2021-08-31T04:53:01.353658] Starting job preparation.\n",
      "[2021-08-31T04:53:01.353685] Extracting the control code.\n",
      "[2021-08-31T04:53:01.353950] Starting extract_project.\n",
      "[2021-08-31T04:53:01.353987] Starting to extract zip file.\n",
      "[2021-08-31T04:53:01.369456] Finished extracting zip file.\n",
      "[2021-08-31T04:53:01.372241] Using urllib.request Python 3.0 or later\n",
      "[2021-08-31T04:53:01.372282] Start fetching snapshots.\n",
      "[2021-08-31T04:53:01.372309] Start fetching snapshot.\n",
      "[2021-08-31T04:53:01.372329] Retrieving project from snapshot: d884c7ef-54ae-4b5a-b3ec-eec1a5fc58f9\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 53\n",
      "[2021-08-31T04:53:01.949344] Finished fetching snapshot.\n",
      "[2021-08-31T04:53:01.949373] Finished fetching snapshots.\n",
      "[2021-08-31T04:53:01.949384] Finished extract_project.\n",
      "[2021-08-31T04:53:01.949429] Finished fetching and extracting the control code.\n",
      "[2021-08-31T04:53:01.955006] Start run_history_prep.\n",
      "[2021-08-31T04:53:01.960374] Job preparation is complete.\n",
      "[2021-08-31T04:53:01.960492] Entering Data Context Managers in Sidecar\n",
      "[2021-08-31T04:53:01.961066] Running Sidecar prep cmd...\n",
      "[2021-08-31T04:53:02.311423] INFO azureml.sidecar.sidecar: Received task: enter_contexts. Running on Linux at /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/ws01/azureml/tf_remote_experiment_1630385291_61b6af45/wd/azureml/tf_remote_experiment_1630385291_61b6af45\n",
      "[2021-08-31T04:53:02.311980] INFO azureml.sidecar.sidecar: Invoking \"enter_contexts\" task with Context Managers: {\"context_managers\": [\"Dataset:context_managers.Datasets\"]}\n",
      "Enter __enter__ of DatasetContextManager\n",
      "SDK version: azureml-core==1.32.0 azureml-dataprep==2.20.1. Session id: 79f5ec84-f5e4-4c35-a4a2-ae0bbc25cb53. Run id: tf_remote_experiment_1630385291_61b6af45.\n",
      "Processing 'input__7c3a3d10'.\n",
      "Mode: 'mount'.\n",
      "Path on compute is specified: 'False'.\n",
      "Processing dataset FileDataset\n",
      "{\n",
      "  \"source\": [\n",
      "    \"('workspaceblobstore', 'tfdata2')\"\n",
      "  ],\n",
      "  \"definition\": [\n",
      "    \"GetDatastoreFiles\"\n",
      "  ],\n",
      "  \"registration\": {\n",
      "    \"id\": \"7c3a3d10-fdfe-4246-97fb-f9a7c499983d\",\n",
      "    \"name\": \"mnist_tfrecords_dataset\",\n",
      "    \"version\": 2,\n",
      "    \"description\": \"training and test dataset\",\n",
      "    \"workspace\": \"Workspace.create(name='ws01', subscription_id='b3ae1c15-4fef-4362-8c3a-5d804cdeb18d', resource_group='TESTML-rg')\"\n",
      "  }\n",
      "}\n",
      "Mounting input__7c3a3d10 to /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/ws01/azureml/tf_remote_experiment_1630385291_61b6af45/wd/input__7c3a3d10_7c3a3d10-fdfe-4246-97fb-f9a7c499983d.\n",
      "Mounted input__7c3a3d10 to /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/ws01/azureml/tf_remote_experiment_1630385291_61b6af45/wd/input__7c3a3d10_7c3a3d10-fdfe-4246-97fb-f9a7c499983d as folder.\n",
      "Exit __enter__ of DatasetContextManager\n",
      "Set Dataset input__7c3a3d10's target path to /mnt/batch/tasks/shared/LS_root/jobs/ws01/azureml/tf_remote_experiment_1630385291_61b6af45/wd/input__7c3a3d10_7c3a3d10-fdfe-4246-97fb-f9a7c499983d\n",
      "[2021-08-31T04:53:08.763580] INFO azureml.sidecar.task.enter_contexts: Entered Context Managers\n",
      "[2021-08-31T04:53:09.525652] Ran Sidecar prep cmd.\n",
      "[2021-08-31T04:53:09.525733] Running Context Managers in Sidecar complete.\n",
      "\n",
      "Streaming azureml-logs/75_job_post-tvmps_068e554c6f1a43ff964b2ce12414d42529985df21a7f868dc4c015ca790b955e_d.txt\n",
      "===============================================================================================================\n",
      "\n",
      "[2021-08-31T04:55:22.170608] Entering job release\n",
      "[2021-08-31T04:55:22.917351] Starting job release\n",
      "[2021-08-31T04:55:22.917774] Logging experiment finalizing status in history service.[2021-08-31T04:55:22.917987] job release stage : upload_datastore starting...\n",
      "\n",
      "[2021-08-31T04:55:22.918289] job release stage : start importing azureml.history._tracking in run_history_release.Starting the daemon thread to refresh tokens in background for process with pid = 320\n",
      "\n",
      "[2021-08-31T04:55:22.919052] job release stage : execute_job_release starting...[2021-08-31T04:55:22.919226] job release stage : copy_batchai_cached_logs starting...\n",
      "\n",
      "[2021-08-31T04:55:22.923734] job release stage : copy_batchai_cached_logs completed...\n",
      "[2021-08-31T04:55:22.931214] Entering context manager injector.\n",
      "[2021-08-31T04:55:22.935750] job release stage : upload_datastore completed...\n",
      "[2021-08-31T04:55:23.008736] job release stage : send_run_telemetry starting...\n",
      "[2021-08-31T04:55:23.019762] get vm size and vm region successfully.\n",
      "[2021-08-31T04:55:23.029205] get compute meta data successfully.\n",
      "[2021-08-31T04:55:23.122124] job release stage : execute_job_release completed...\n",
      "[2021-08-31T04:55:23.294786] post artifact meta request successfully.\n",
      "[2021-08-31T04:55:23.317531] upload compute record artifact successfully.\n",
      "[2021-08-31T04:55:23.317596] job release stage : send_run_telemetry completed...\n",
      "[2021-08-31T04:55:23.317820] Running in AzureML-Sidecar, starting to exit user context managers...\n",
      "[2021-08-31T04:55:23.318000] Running Sidecar release cmd...\n",
      "[2021-08-31T04:55:23.328071] INFO azureml.sidecar.sidecar: Received task: exit_contexts. Running on Linux at /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/ws01/azureml/tf_remote_experiment_1630385291_61b6af45/wd/azureml/tf_remote_experiment_1630385291_61b6af45\n",
      "Enter __exit__ of DatasetContextManager\n",
      "Unmounting /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/ws01/azureml/tf_remote_experiment_1630385291_61b6af45/wd/input__7c3a3d10_7c3a3d10-fdfe-4246-97fb-f9a7c499983d.\n",
      "fuse: failed to unmount /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/ws01/azureml/tf_remote_experiment_1630385291_61b6af45/wd/input__7c3a3d10_7c3a3d10-fdfe-4246-97fb-f9a7c499983d: Invalid argument\n",
      "Finishing unmounting /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/ws01/azureml/tf_remote_experiment_1630385291_61b6af45/wd/input__7c3a3d10_7c3a3d10-fdfe-4246-97fb-f9a7c499983d.\n",
      "Exit __exit__ of DatasetContextManager\n",
      "[2021-08-31T04:55:23.361994] Removing absolute paths from host...\n",
      "[2021-08-31T04:55:23.362202] INFO azureml.sidecar.task.exit_contexts: Exited Context Managers\n",
      "[2021-08-31T04:55:23.937283] Ran Sidecar release cmd.\n",
      "[2021-08-31T04:55:23.937362] Job release is complete\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: tf_remote_experiment_1630385291_61b6af45\n",
      "Web View: https://ml.azure.com/runs/tf_remote_experiment_1630385291_61b6af45?wsid=/subscriptions/b3ae1c15-4fef-4362-8c3a-5d804cdeb18d/resourcegroups/TESTML-rg/workspaces/ws01&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'runId': 'tf_remote_experiment_1630385291_61b6af45',\n",
       " 'target': 'mydsvm01',\n",
       " 'status': 'Completed',\n",
       " 'startTimeUtc': '2021-08-31T04:52:33.500864Z',\n",
       " 'endTimeUtc': '2021-08-31T04:55:31.773239Z',\n",
       " 'properties': {'_azureml.ComputeTargetType': 'amlcompute',\n",
       "  'ContentSnapshotId': 'd884c7ef-54ae-4b5a-b3ec-eec1a5fc58f9',\n",
       "  'ProcessInfoFile': 'azureml-logs/process_info.json',\n",
       "  'ProcessStatusFile': 'azureml-logs/process_status.json',\n",
       "  'azureml.git.repository_uri': 'https://github.com/tsmatz/azureml-tutorial-tensorflow-v1.git',\n",
       "  'mlflow.source.git.repoURL': 'https://github.com/tsmatz/azureml-tutorial-tensorflow-v1.git',\n",
       "  'azureml.git.branch': 'master',\n",
       "  'mlflow.source.git.branch': 'master',\n",
       "  'azureml.git.commit': '9a48ad294217dbd8be2f52f124ecc0d9d68bad6c',\n",
       "  'mlflow.source.git.commit': '9a48ad294217dbd8be2f52f124ecc0d9d68bad6c',\n",
       "  'azureml.git.dirty': 'True'},\n",
       " 'inputDatasets': [{'dataset': {'id': '7c3a3d10-fdfe-4246-97fb-f9a7c499983d'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'input__7c3a3d10', 'mechanism': 'Mount'}}],\n",
       " 'outputDatasets': [],\n",
       " 'runDefinition': {'script': 'train.py',\n",
       "  'command': '',\n",
       "  'useAbsolutePath': False,\n",
       "  'arguments': ['--data_folder', 'DatasetConsumptionConfig:input__7c3a3d10'],\n",
       "  'sourceDirectoryDataStore': None,\n",
       "  'framework': 'Python',\n",
       "  'communicator': 'None',\n",
       "  'target': 'mydsvm01',\n",
       "  'dataReferences': {},\n",
       "  'data': {'input__7c3a3d10': {'dataLocation': {'dataset': {'id': '7c3a3d10-fdfe-4246-97fb-f9a7c499983d',\n",
       "      'name': 'mnist_tfrecords_dataset',\n",
       "      'version': '2'},\n",
       "     'dataPath': None,\n",
       "     'uri': None},\n",
       "    'mechanism': 'Mount',\n",
       "    'environmentVariableName': 'input__7c3a3d10',\n",
       "    'pathOnCompute': None,\n",
       "    'overwrite': False,\n",
       "    'options': None}},\n",
       "  'outputData': {},\n",
       "  'datacaches': [],\n",
       "  'jobName': None,\n",
       "  'maxRunDurationSeconds': 2592000,\n",
       "  'nodeCount': 1,\n",
       "  'priority': None,\n",
       "  'credentialPassthrough': False,\n",
       "  'identity': None,\n",
       "  'environment': {'name': 'test-remote-gpu-env',\n",
       "   'version': '1',\n",
       "   'python': {'interpreterPath': 'python',\n",
       "    'userManagedDependencies': False,\n",
       "    'condaDependencies': {'channels': ['anaconda', 'conda-forge'],\n",
       "     'dependencies': ['python=3.6',\n",
       "      {'pip': ['azureml-defaults~=1.33.0']},\n",
       "      'tensorflow-gpu==1.15'],\n",
       "     'name': 'azureml_02cb4dd66a36882cd7876a87ad6f1407'},\n",
       "    'baseCondaEnvironment': None},\n",
       "   'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'},\n",
       "   'docker': {'baseImage': 'mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.2-cudnn8-ubuntu18.04:20210714.v1',\n",
       "    'platform': {'os': 'Linux', 'architecture': 'amd64'},\n",
       "    'baseDockerfile': None,\n",
       "    'baseImageRegistry': {'address': None, 'username': None, 'password': None},\n",
       "    'enabled': False,\n",
       "    'arguments': []},\n",
       "   'spark': {'repositories': [], 'packages': [], 'precachePackages': True},\n",
       "   'inferencingStackVersion': None},\n",
       "  'history': {'outputCollection': True,\n",
       "   'directoriesToWatch': ['logs'],\n",
       "   'enableMLflowTracking': True,\n",
       "   'snapshotProject': True},\n",
       "  'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment',\n",
       "    'spark.yarn.maxAppAttempts': '1'}},\n",
       "  'parallelTask': {'maxRetriesPerWorker': 0,\n",
       "   'workerCountPerNode': 1,\n",
       "   'terminalExitCodes': None,\n",
       "   'configuration': {}},\n",
       "  'amlCompute': {'name': None,\n",
       "   'vmSize': None,\n",
       "   'retainCluster': False,\n",
       "   'clusterMaxNodeCount': None},\n",
       "  'aiSuperComputer': {'instanceType': 'D2',\n",
       "   'imageVersion': 'pytorch-1.7.0',\n",
       "   'location': None,\n",
       "   'aiSuperComputerStorageData': None,\n",
       "   'interactive': False,\n",
       "   'scalePolicy': None,\n",
       "   'virtualClusterArmId': None,\n",
       "   'tensorboardLogDirectory': None,\n",
       "   'sshPublicKey': None,\n",
       "   'enableAzmlInt': True,\n",
       "   'priority': 'Medium',\n",
       "   'slaTier': 'Standard',\n",
       "   'userAlias': None},\n",
       "  'kubernetesCompute': {'instanceType': None},\n",
       "  'tensorflow': {'workerCount': 1, 'parameterServerCount': 1},\n",
       "  'mpi': {'processCountPerNode': 1},\n",
       "  'pyTorch': {'communicationBackend': 'nccl', 'processCount': None},\n",
       "  'hdi': {'yarnDeployMode': 'Cluster'},\n",
       "  'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5},\n",
       "  'exposedPorts': None,\n",
       "  'docker': {'useDocker': True,\n",
       "   'sharedVolumes': True,\n",
       "   'shmSize': '2g',\n",
       "   'arguments': []},\n",
       "  'cmk8sCompute': {'configuration': {}},\n",
       "  'commandReturnCodeConfig': {'returnCode': 'Zero',\n",
       "   'successfulReturnCodes': []},\n",
       "  'environmentVariables': {},\n",
       "  'applicationEndpoints': {},\n",
       "  'parameters': [],\n",
       "  'dataBricks': {'workers': 0,\n",
       "   'minimumWorkerCount': 0,\n",
       "   'maxMumWorkerCount': 0,\n",
       "   'sparkVersion': '4.0.x-scala2.11',\n",
       "   'nodeTypeId': 'Standard_D3_v2',\n",
       "   'sparkConf': {},\n",
       "   'sparkEnvVars': {},\n",
       "   'instancePoolId': None,\n",
       "   'timeoutSeconds': 0,\n",
       "   'jarLibraries': [],\n",
       "   'eggLibraries': [],\n",
       "   'whlLibraries': [],\n",
       "   'pypiLibraries': [],\n",
       "   'rCranLibraries': [],\n",
       "   'mavenLibraries': []}},\n",
       " 'logFiles': {'azureml-logs/55_azureml-execution-tvmps_068e554c6f1a43ff964b2ce12414d42529985df21a7f868dc4c015ca790b955e_d.txt': 'https://ws016372024076.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_remote_experiment_1630385291_61b6af45/azureml-logs/55_azureml-execution-tvmps_068e554c6f1a43ff964b2ce12414d42529985df21a7f868dc4c015ca790b955e_d.txt?sv=2019-07-07&sr=b&sig=8IuIiadscNXIscpDOTRkHs8CJTiSAupsgZITpq8sVe0%3D&st=2021-08-31T04%3A45%3A41Z&se=2021-08-31T12%3A55%3A41Z&sp=r',\n",
       "  'azureml-logs/65_job_prep-tvmps_068e554c6f1a43ff964b2ce12414d42529985df21a7f868dc4c015ca790b955e_d.txt': 'https://ws016372024076.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_remote_experiment_1630385291_61b6af45/azureml-logs/65_job_prep-tvmps_068e554c6f1a43ff964b2ce12414d42529985df21a7f868dc4c015ca790b955e_d.txt?sv=2019-07-07&sr=b&sig=Kf4%2FBu%2FY1t1i%2BPVMvaFxbWsvLeoApEMa9e3ZiVeRHm0%3D&st=2021-08-31T04%3A45%3A41Z&se=2021-08-31T12%3A55%3A41Z&sp=r',\n",
       "  'azureml-logs/70_driver_log.txt': 'https://ws016372024076.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_remote_experiment_1630385291_61b6af45/azureml-logs/70_driver_log.txt?sv=2019-07-07&sr=b&sig=1d64h4IriiI1SeWXqa3R3b63%2F9oNJo25D4a2qIxxdBE%3D&st=2021-08-31T04%3A45%3A41Z&se=2021-08-31T12%3A55%3A41Z&sp=r',\n",
       "  'azureml-logs/75_job_post-tvmps_068e554c6f1a43ff964b2ce12414d42529985df21a7f868dc4c015ca790b955e_d.txt': 'https://ws016372024076.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_remote_experiment_1630385291_61b6af45/azureml-logs/75_job_post-tvmps_068e554c6f1a43ff964b2ce12414d42529985df21a7f868dc4c015ca790b955e_d.txt?sv=2019-07-07&sr=b&sig=LYvLWlvr4U1Cdp54023gA5CrjYado9s%2Ft23av4OZZGA%3D&st=2021-08-31T04%3A45%3A41Z&se=2021-08-31T12%3A55%3A41Z&sp=r',\n",
       "  'azureml-logs/process_info.json': 'https://ws016372024076.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_remote_experiment_1630385291_61b6af45/azureml-logs/process_info.json?sv=2019-07-07&sr=b&sig=dznuD1QP%2BJ5kGpDMBjlZmqaKeT%2F2eQ2n6cxQ1AnIvNA%3D&st=2021-08-31T04%3A45%3A41Z&se=2021-08-31T12%3A55%3A41Z&sp=r',\n",
       "  'azureml-logs/process_status.json': 'https://ws016372024076.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_remote_experiment_1630385291_61b6af45/azureml-logs/process_status.json?sv=2019-07-07&sr=b&sig=Np%2BJ90cGIKOX0pIUl3zIZKvT5D9PPYk%2BlMcgoUuCLWs%3D&st=2021-08-31T04%3A45%3A41Z&se=2021-08-31T12%3A55%3A41Z&sp=r',\n",
       "  'logs/azureml/dataprep/backgroundProcess.log': 'https://ws016372024076.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_remote_experiment_1630385291_61b6af45/logs/azureml/dataprep/backgroundProcess.log?sv=2019-07-07&sr=b&sig=K83JkrKisl8WLmrDH9ySAvuHOA0WEKIu9DYjDiWg9w0%3D&st=2021-08-31T04%3A45%3A41Z&se=2021-08-31T12%3A55%3A41Z&sp=r',\n",
       "  'logs/azureml/dataprep/backgroundProcess_Telemetry.log': 'https://ws016372024076.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_remote_experiment_1630385291_61b6af45/logs/azureml/dataprep/backgroundProcess_Telemetry.log?sv=2019-07-07&sr=b&sig=VvkPA6Q%2BgRkHO4E7RWzjKgW92EU7nv0Ho0VzrjaSJPE%3D&st=2021-08-31T04%3A45%3A41Z&se=2021-08-31T12%3A55%3A41Z&sp=r',\n",
       "  'logs/azureml/job_release_azureml.log': 'https://ws016372024076.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_remote_experiment_1630385291_61b6af45/logs/azureml/job_release_azureml.log?sv=2019-07-07&sr=b&sig=fyDu0xL0e2q3iMRwZBMAiscFnSUQkL5JAoiG3SefJl4%3D&st=2021-08-31T04%3A45%3A41Z&se=2021-08-31T12%3A55%3A41Z&sp=r',\n",
       "  'logs/azureml/sidecar/tvmps_068e554c6f1a43ff964b2ce12414d42529985df21a7f868dc4c015ca790b955e_d/all.log': 'https://ws016372024076.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_remote_experiment_1630385291_61b6af45/logs/azureml/sidecar/tvmps_068e554c6f1a43ff964b2ce12414d42529985df21a7f868dc4c015ca790b955e_d/all.log?sv=2019-07-07&sr=b&sig=D87JipcavmzipycGbABZw%2F8iIJDEnLP6x2RDW4l7%2F88%3D&st=2021-08-31T04%3A45%3A41Z&se=2021-08-31T12%3A55%3A41Z&sp=r',\n",
       "  'logs/azureml/sidecar/tvmps_068e554c6f1a43ff964b2ce12414d42529985df21a7f868dc4c015ca790b955e_d/task.exit_contexts.log': 'https://ws016372024076.blob.core.windows.net/azureml/ExperimentRun/dcid.tf_remote_experiment_1630385291_61b6af45/logs/azureml/sidecar/tvmps_068e554c6f1a43ff964b2ce12414d42529985df21a7f868dc4c015ca790b955e_d/task.exit_contexts.log?sv=2019-07-07&sr=b&sig=fj89sYPQD6IY0SUam%2FQTB%2F3hTj84Nf6KIbKneAbDNQM%3D&st=2021-08-31T04%3A45%3A41Z&se=2021-08-31T12%3A55%3A41Z&sp=r'},\n",
       " 'submittedBy': 'Tsuyoshi Matsuzaki'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.core import Run\n",
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.core.runconfig import DockerConfiguration\n",
    "\n",
    "# create script run config\n",
    "src = ScriptRunConfig(\n",
    "    source_directory='./script',\n",
    "    script='train.py',\n",
    "    arguments=['--data_folder', dataset.as_mount()],\n",
    "    compute_target=compute_target,\n",
    "    environment=env,\n",
    "    docker_runtime_config=DockerConfiguration(use_docker=True))\n",
    "\n",
    "# submit and run !\n",
    "exp = Experiment(workspace=ws, name='tf_remote_experiment')\n",
    "run = exp.submit(config=src)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 : Download results and evaluate\n",
    "\n",
    "Now let's check the generated model in local computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, check generated files and logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['azureml-logs/55_azureml-execution-tvmps_068e554c6f1a43ff964b2ce12414d42529985df21a7f868dc4c015ca790b955e_d.txt',\n",
       " 'azureml-logs/65_job_prep-tvmps_068e554c6f1a43ff964b2ce12414d42529985df21a7f868dc4c015ca790b955e_d.txt',\n",
       " 'azureml-logs/70_driver_log.txt',\n",
       " 'azureml-logs/75_job_post-tvmps_068e554c6f1a43ff964b2ce12414d42529985df21a7f868dc4c015ca790b955e_d.txt',\n",
       " 'azureml-logs/process_info.json',\n",
       " 'azureml-logs/process_status.json',\n",
       " 'logs/azureml/dataprep/backgroundProcess.log',\n",
       " 'logs/azureml/dataprep/backgroundProcess_Telemetry.log',\n",
       " 'logs/azureml/job_release_azureml.log',\n",
       " 'logs/azureml/sidecar/tvmps_068e554c6f1a43ff964b2ce12414d42529985df21a7f868dc4c015ca790b955e_d/all.log',\n",
       " 'logs/azureml/sidecar/tvmps_068e554c6f1a43ff964b2ce12414d42529985df21a7f868dc4c015ca790b955e_d/task.exit_contexts.log',\n",
       " 'logs/checkpoint',\n",
       " 'logs/eval/events.out.tfevents.1630385719.50eec89ecf714ceb9fd036b65baa0762000001',\n",
       " 'logs/events.out.tfevents.1630385704.50eec89ecf714ceb9fd036b65baa0762000001',\n",
       " 'logs/graph.pbtxt',\n",
       " 'logs/model.ckpt-0.data-00000-of-00001',\n",
       " 'logs/model.ckpt-0.index',\n",
       " 'logs/model.ckpt-0.meta',\n",
       " 'logs/model.ckpt-1100.data-00000-of-00001',\n",
       " 'logs/model.ckpt-1100.index',\n",
       " 'logs/model.ckpt-1100.meta',\n",
       " 'outputs/1630385719/saved_model.pb',\n",
       " 'outputs/1630385719/variables/variables.data-00000-of-00001',\n",
       " 'outputs/1630385719/variables/variables.index']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.get_file_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download model into your local machine.    \n",
    "**Please change ```1630385719``` to meet previous results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.download_file(\n",
    "    name='outputs/1630385719/saved_model.pb',\n",
    "    output_file_path='remote_model/saved_model.pb')\n",
    "run.download_file(\n",
    "    name='outputs/1630385719/variables/variables.data-00000-of-00001',\n",
    "    output_file_path='remote_model/variables/variables.data-00000-of-00001')\n",
    "run.download_file(\n",
    "    name='outputs/1630385719/variables/variables.index',\n",
    "    output_file_path='remote_model/variables/variables.index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict your test data using downloaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda/envs/myenv/lib/python3.6/site-packages/tensorflow_core/contrib/predictor/saved_model_predictor.py:153: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "INFO:tensorflow:Restoring parameters from ./remote_model/variables/variables\n",
      "Predicted:  [7, 2, 1]\n",
      "Actual   :  [7, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Read data by tensor\n",
    "tfdata = tf.data.TFRecordDataset('./data/test.tfrecords')\n",
    "iterator = tf.compat.v1.data.make_one_shot_iterator(tfdata)\n",
    "data_org = iterator.get_next()\n",
    "data_exam = tf.parse_single_example(\n",
    "    data_org,\n",
    "    features={\n",
    "        'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "        'label': tf.FixedLenFeature([], tf.int64)\n",
    "    })\n",
    "data_image = tf.decode_raw(data_exam['image_raw'], tf.uint8)\n",
    "data_image.set_shape([784])\n",
    "data_image = tf.cast(data_image, tf.float32) * (1. / 255)\n",
    "data_label = tf.cast(data_exam['label'], tf.int32)\n",
    "\n",
    "# Run tensor and generate data\n",
    "with tf.Session() as sess:\n",
    "    image_arr = []\n",
    "    label_arr = []\n",
    "    for i in range(3):\n",
    "        image, label = sess.run([data_image, data_label])\n",
    "        image_arr.append(image)\n",
    "        label_arr.append(label)\n",
    "\n",
    "# Predict\n",
    "pred_fn = tf.contrib.predictor.from_saved_model('./remote_model')\n",
    "pred = pred_fn({'inputs': image_arr})\n",
    "\n",
    "print('Predicted: ', pred['classes'].tolist())\n",
    "print('Actual   : ', label_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7 : Register Model with Dataset reference\n",
    "\n",
    "By registering model with dataset reference, you can trace the model with the corresponding dataset version.<br>\n",
    "(**Please change ```1629700431``` to meet previous results.**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = run.register_model(\n",
    "    model_name='mnist_model_test',\n",
    "    model_path='outputs/1630385719',\n",
    "    datasets =[('training data',dataset)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to track data used in this model, see this model in [Azure Machine Learning Studio](https://ml.azure.com/) and select \"Datasets\" tab. (See the following screenshot.)\n",
    "\n",
    "![data tracking](https://tsmatz.files.wordpress.com/2021/08/20210823_track_data.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8 : Remove AML compute\n",
    "\n",
    "**You don't need to remove your AML compute** for saving money, because the nodes will be automatically terminated, when it's inactive.    \n",
    "But if you want to clean up, please run the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete cluster (nbodes) and remove from AML workspace\n",
    "mycompute = AmlCompute(workspace=ws, name='myvm01')\n",
    "mycompute.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'currentNodeCount': 1, 'targetNodeCount': 1, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 1, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2021-08-31T04:52:16.858000+00:00', 'errors': None, 'creationTime': '2021-08-31T04:45:45.747268+00:00', 'modifiedTime': '2021-08-31T04:46:11.291359+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 1, 'nodeIdleTimeBeforeScaleDown': 'PT1800S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_NC4AS_T4_V3'}\n"
     ]
    }
   ],
   "source": [
    "# get a status for the current cluster.\n",
    "print(mycompute.status.serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
