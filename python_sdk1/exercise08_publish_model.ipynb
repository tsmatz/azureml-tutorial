{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise08 : Publish as a Web Service\n",
    "\n",
    "Finally we publish our model as a web service.\n",
    "\n",
    "Before running this code, **complete the model registration in \"[Exercise04 : Train on Remote GPU Virtual Machine](./exercise04_train_remote.ipynb)\"**.\n",
    "\n",
    "*back to [index](https://github.com/tsmatz/azureml-tutorial/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get workspace settings\n",
    "\n",
    "Before starting, you must read your configuration settings. (See \"[Exercise01 : Prepare Config Settings](./exercise01_prepare_config.ipynb)\".)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "import azureml.core\n",
    "\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a registered model\n",
    "\n",
    "Get the trained model by MNIST dataset.<br>\n",
    "**Before running this code, complete the model training in \"[Exercise04 : Train on Remote GPU Virtual Machine](./exercise04_train_remote.ipynb)\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "registered_model = Model(ws, 'mnist_model_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create entry script (.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to deploy as web service, first we generate the following scoring code.<br>\n",
    "This entry script in AML should include both ```init()``` and ```run()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "script_folder = './inference_script'\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing inference_script/score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference_script/score.py\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from azureml.core.model import Model\n",
    "\n",
    "def init():\n",
    "    global loaded_model\n",
    "    model_path = Model.get_model_path(model_name='mnist_model_test')\n",
    "    loaded_model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "def run(raw_data):\n",
    "    try:\n",
    "        data = json.loads(raw_data)[\"data\"]\n",
    "        pred_output = loaded_model(np.array(data))\n",
    "        pred_list = tf.math.argmax(pred_output, axis=-1).numpy().tolist()\n",
    "        return pred_list\n",
    "    except Exception as e:\n",
    "       result = str(e)\n",
    "       return 'Internal Exception : ' + result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy as web service\n",
    "\n",
    "Here we deploy a registered model as web service.\n",
    "\n",
    "> Note : When you build a container image without deploying (such as, for deploying model on Edge devices), package a registered model with Docker. (See [here](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-package-models).)<br>\n",
    "> For debugging purpose, you can also deploy a model on your local computer running docker runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create deploy configuration for preparation.<br>\n",
    "In this tutorial, we will deploy our serving in Azure Container Instance (ACI) with anonymous access permission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice, Webservice\n",
    "\n",
    "aci_conf = AciWebservice.deploy_configuration(\n",
    "    cpu_cores=1,\n",
    "    memory_gb=1, \n",
    "    description='This is a tensorflow example.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create inference configuration for preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.environment import Environment\n",
    "\n",
    "# Generate conda dependency\n",
    "conda_dependency = CondaDependencies.create()\n",
    "conda_dependency.add_pip_package('tensorflow==2.10.0')\n",
    "conda_dependency.add_pip_package('numpy')\n",
    "### Or you can also write as follows (make sure to insert 'azureml-defaults' module)\n",
    "#conda_dependency = CondaDependencies.create(pip_packages=['azureml-defaults', 'tensorflow==2.10.0'])\n",
    "\n",
    "# Create environment and set the previous conda dependency\n",
    "myenv = Environment(name=\"myenv\")\n",
    "myenv.python.conda_dependencies = conda_dependency\n",
    "\n",
    "# Create inference config with score.py\n",
    "inf_conf = InferenceConfig(\n",
    "    entry_script=\"score.py\",\n",
    "    source_directory='./inference_script',\n",
    "    environment=myenv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, deploy as a web service !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24042/3878658173.py:1: FutureWarning: azureml.core.model:\n",
      "To leverage new model deployment capabilities, AzureML recommends using CLI/SDK v2 to deploy models as online endpoint, \n",
      "please refer to respective documentations \n",
      "https://docs.microsoft.com/azure/machine-learning/how-to-deploy-managed-online-endpoints /\n",
      "https://docs.microsoft.com/azure/machine-learning/how-to-deploy-managed-online-endpoint-sdk-v2 /\n",
      "https://docs.microsoft.com/azure/machine-learning/how-to-attach-kubernetes-anywhere \n",
      "For more information on migration, see https://aka.ms/acimoemigration. \n",
      "To disable CLI/SDK v1 deprecation warning set AZUREML_LOG_DEPRECATION_WARNING_ENABLED to 'False'\n",
      "  svc = Model.deploy(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
      "Running\n",
      "2022-10-05 06:54:16+00:00 Creating Container Registry if not exists.\n",
      "2022-10-05 06:54:16+00:00 Registering the environment.\n",
      "2022-10-05 06:54:17+00:00 Building image..\n",
      "2022-10-05 07:04:31+00:00 Generating deployment configuration..\n",
      "2022-10-05 07:04:32+00:00 Submitting deployment to compute.\n",
      "2022-10-05 07:04:37+00:00 Checking the status of deployment my-mnist-service..\n",
      "2022-10-05 07:07:18+00:00 Checking the status of inference endpoint my-mnist-service.\n",
      "Succeeded\n",
      "ACI service creation operation finished, operation \"Succeeded\"\n"
     ]
    }
   ],
   "source": [
    "svc = Model.deploy(\n",
    "    name='my-mnist-service',\n",
    "    deployment_config=aci_conf,\n",
    "    models=[registered_model],\n",
    "    inference_config=inf_conf,\n",
    "    workspace=ws)\n",
    "svc.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-23T08:25:17,379725500+00:00 - iot-server/run \n",
      "2021-08-23T08:25:17,386434100+00:00 - rsyslog/run \n",
      "2021-08-23T08:25:17,395805700+00:00 - gunicorn/run \n",
      "Dynamic Python package installation is disabled.\n",
      "Starting HTTP server\n",
      "2021-08-23T08:25:17,398135200+00:00 - nginx/run \n",
      "EdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n",
      "2021-08-23T08:25:17,764669100+00:00 - iot-server/finish 1 0\n",
      "2021-08-23T08:25:17,770766800+00:00 - Exit code 1 is normal. Not restarting iot-server.\n",
      "Starting gunicorn 20.1.0\n",
      "Listening at: http://127.0.0.1:31311 (65)\n",
      "Using worker: sync\n",
      "worker timeout is set to 300\n",
      "Booting worker with pid: 93\n",
      "SPARK_HOME not set. Skipping PySpark Initialization.\n",
      "Initializing logger\n",
      "2021-08-23 08:25:23,364 | root | INFO | Starting up app insights client\n",
      "logging socket was found. logging is available.\n",
      "logging socket was found. logging is available.\n",
      "2021-08-23 08:25:23,364 | root | INFO | Starting up request id generator\n",
      "2021-08-23 08:25:23,364 | root | INFO | Starting up app insight hooks\n",
      "2021-08-23 08:25:23,364 | root | INFO | Invoking user's init function\n",
      "\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "2021-08-23 08:25:24.420186: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /azureml-envs/azureml_f2a613bfe61b2fc9ece8b47008c58af5/lib:/azureml-envs/azureml_f2a613bfe61b2fc9ece8b47008c58af5/lib:\n",
      "2021-08-23 08:25:24.420287: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-08-23 08:25:24.420321: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (SandboxHost-637653037711236532): /proc/driver/nvidia/version does not exist\n",
      "2021-08-23 08:25:24.420666: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "2021-08-23 08:25:24.443225: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095074999 Hz\n",
      "2021-08-23 08:25:24.443914: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55dd35e30250 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-08-23 08:25:24.444074: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "From /azureml-envs/azureml_f2a613bfe61b2fc9ece8b47008c58af5/lib/python3.6/site-packages/tensorflow_core/contrib/predictor/saved_model_predictor.py:153: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "Restoring parameters from azureml-models/mnist_model_test/1/1629700431/variables/variables\n",
      "2021-08-23 08:25:24,546 | root | INFO | Users's init has completed successfully\n",
      "2021-08-23 08:25:24,550 | root | INFO | Skipping middleware: dbg_model_info as it's not enabled.\n",
      "2021-08-23 08:25:24,550 | root | INFO | Skipping middleware: dbg_resource_usage as it's not enabled.\n",
      "2021-08-23 08:25:24,554 | root | INFO | Scoring timeout is found from os.environ: 60000 ms\n",
      "2021-08-23 08:25:41,887 | root | INFO | Swagger file not present\n",
      "2021-08-23 08:25:41,887 | root | INFO | 404\n",
      "127.0.0.1 - - [23/Aug/2021:08:25:41 +0000] \"GET /swagger.json HTTP/1.0\" 404 19 \"-\" \"Go-http-client/1.1\"\n",
      "2021-08-23 08:25:47,661 | root | INFO | Swagger file not present\n",
      "2021-08-23 08:25:47,662 | root | INFO | 404\n",
      "127.0.0.1 - - [23/Aug/2021:08:25:47 +0000] \"GET /swagger.json HTTP/1.0\" 404 19 \"-\" \"Go-http-client/1.1\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See details, if error has occured\n",
    "print(svc.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check service url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://43e695df-70bd-4d61-88e4-d15dd23d45ec.eastus.azurecontainer.io/score'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.scoring_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your web service\n",
    "\n",
    "Let's invoke your web service and check the returned results in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-05 07:47:34.497224: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-05 07:47:34.648137: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-05 07:47:34.648169: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-05 07:47:34.681609: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-05 07:47:35.441531: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-05 07:47:35.441640: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-05 07:47:35.441652: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted :  [7, 2, 1]\n",
      "Actual    :  [7, 2, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-05 07:47:35.976473: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-10-05 07:47:35.976516: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-05 07:47:35.976547: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (client1005): /proc/driver/nvidia/version does not exist\n",
      "2022-10-05 07:47:35.976776: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Read data by tensor\n",
    "test_data = tf.data.Dataset.load(\"./data/test\")\n",
    "\n",
    "# Generate data\n",
    "image_arr = []\n",
    "label_arr = []\n",
    "for image, label in test_data.take(3):\n",
    "    image_arr.append(image.numpy().tolist())\n",
    "    label_arr.append(label.numpy().item())\n",
    "\n",
    "# Invoke web service !\n",
    "headers = {\n",
    "    'Content-Type':'application/json'\n",
    "} \n",
    "# for AKS deployment (in production), provide service key in the header as below :\n",
    "# api_key1, api_key2 = svc.get_keys()\n",
    "# headers = {'Content-Type':'application/json',  'Authorization':('Bearer '+ api_key1)} \n",
    "values = json.dumps(image_arr)\n",
    "input_data = \"{\\\"data\\\": \" + values + \"}\"\n",
    "http_res = requests.post(\n",
    "    svc.scoring_uri,\n",
    "    input_data,\n",
    "    headers = headers)\n",
    "print('Predicted : ', http_res.text)\n",
    "print('Actual    : ', label_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
