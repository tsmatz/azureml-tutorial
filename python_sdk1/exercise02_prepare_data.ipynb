{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 02 : Prepare Data\n",
    "\n",
    "Here we learn ```Datastore``` and ```Dataset``` in Azure Machine Learning.<br>\n",
    "The subsequent all exercises (Exercise 04 -) will use data provisioned in this exercise, and you should then run this exercise beforehand.\n",
    "\n",
    "Here we use hand-writing digit's dataset ([MNIST](http://yann.lecun.com/exdb/mnist/)) to train in this tutorial.\n",
    "\n",
    "*back to [index](https://github.com/tsmatz/azureml-tutorial/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get config setting\n",
    "\n",
    "Read your config settings. (See and run \"[Exercise01 : Prepare Config Settings](./exercise01_prepare_config.ipynb)\" beforehand.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "import azureml.core\n",
    "\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data in local folder\n",
    "\n",
    "Download MNIST (hand-writing digits) dataset in ```./data``` folder.<br>\n",
    "The generated train data has 60,000 records and test data has 10,000 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-05 04:20:18.196181: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-05 04:20:18.357260: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-05 04:20:18.357293: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-05 04:20:18.390218: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-05 04:20:19.232189: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-05 04:20:19.232305: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-05 04:20:19.232317: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-05 04:20:20.329581: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-10-05 04:20:20.329622: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-05 04:20:20.329658: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (client1005): /proc/driver/nvidia/version does not exist\n",
      "2022-10-05 04:20:20.330044: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels),(test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "\n",
    "train_dataset.save('./data/train')\n",
    "test_dataset.save('./data/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use default datastore\n",
    "\n",
    "Azure Machine Learning (AML) workspace has its own default datastore. When you create an AML workspace, a storage account (default datastore) is automatically generated in the same resource group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get workspace default datastore\n",
    "ds = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and register data as Dataset\n",
    "\n",
    "Now we create AML dataset and register in workspace.\n",
    "\n",
    "The data will be uploaded into the container in storage account, and you can share data in AML workspace.<br>\n",
    "Registering dataset is not mandatory, but you can trace versions of data with models or experiments by registering data as AML dataset. (You can see the registered dataset in AML studio UI.)\n",
    "\n",
    "In this exercise, I register all files in ```data``` folder, but you can also register a part of files (such as, files with specific extension) as dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating arguments.\n",
      "Arguments validated.\n",
      "Uploading file to tfdata\n",
      "Uploading an estimated of 6 files\n",
      "Uploading ./data/test/dataset_spec.pb\n",
      "Uploaded ./data/test/dataset_spec.pb, 1 files out of an estimated total of 6\n",
      "Uploading ./data/test/snapshot.metadata\n",
      "Uploaded ./data/test/snapshot.metadata, 2 files out of an estimated total of 6\n",
      "Uploading ./data/train/dataset_spec.pb\n",
      "Uploaded ./data/train/dataset_spec.pb, 3 files out of an estimated total of 6\n",
      "Uploading ./data/train/snapshot.metadata\n",
      "Uploaded ./data/train/snapshot.metadata, 4 files out of an estimated total of 6\n",
      "Uploading ./data/test/7255580156754328102/00000000.shard/00000000.snapshot\n",
      "Uploaded ./data/test/7255580156754328102/00000000.shard/00000000.snapshot, 5 files out of an estimated total of 6\n",
      "Uploading ./data/train/4093752533441703203/00000000.shard/00000000.snapshot\n",
      "Uploaded ./data/train/4093752533441703203/00000000.shard/00000000.snapshot, 6 files out of an estimated total of 6\n",
      "Uploaded 6 files\n",
      "Creating new dataset\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Dataset\n",
    "from azureml.data.datapath import DataPath\n",
    "\n",
    "# Upload local \"data\" folder (incl. files) as \"tfdata\" folder\n",
    "mnist_dataset = Dataset.File.upload_directory(\n",
    "    src_dir='./data',\n",
    "    target=DataPath(ds, 'tfdata'),\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "# Register dataset\n",
    "mnist_dataset = mnist_dataset.register(\n",
    "    workspace=ws,\n",
    "    name='mnist_dataset',\n",
    "    description='MNIST training and test dataset',\n",
    "    create_new_version=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Optional] Use datastore with your own provisioned storage\n",
    "\n",
    "(Running this tutorial is not needed for the following exercises, and you can skip.)\n",
    "\n",
    "Here we learn how to use your own blob storage as AML datastore.\n",
    "\n",
    "Before running, **please create Azure storage account and container as follows**.\n",
    "\n",
    "1. Create your Storage Account in [Azure Portal](https://portal.azure.com/).\n",
    "2. Create a container in storage account.\n",
    "3. Copy storage account name, access key, and container name.\n",
    "4. Set these values in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nds2 = Datastore.register_azure_blob_container(\\n    ws,\\n    datastore_name='myblob01',\\n    account_name='{STORAGE ACCOUNT NAME}',\\n    account_key='{ACCESS KEY}',\\n    container_name='{CONTAINER NAME}',\\n    overwrite=True)\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core import Datastore\n",
    "\n",
    "# Register your own storage as AML datastore\n",
    "ds2 = Datastore.register_azure_blob_container(\n",
    "    ws,\n",
    "    datastore_name='myblob01',\n",
    "    account_name='{STORAGE ACCOUNT NAME}',\n",
    "    account_key='{ACCESS KEY}',\n",
    "    container_name='{CONTAINER NAME}',\n",
    "    overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have registered your own datastore, you can use this datastore with familiar API.<br>\n",
    "In this example, I upload local data. (See the uploaded data in your storage account.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating arguments.\n",
      "Arguments validated.\n",
      "Uploading file to tfdata\n",
      "Uploading an estimated of 2 files\n",
      "Uploading ./data/test.tfrecords\n",
      "Uploaded ./data/test.tfrecords, 1 files out of an estimated total of 2\n",
      "Uploading ./data/train.tfrecords\n",
      "Uploaded ./data/train.tfrecords, 2 files out of an estimated total of 2\n",
      "Uploaded 2 files\n",
      "Creating new dataset\n"
     ]
    }
   ],
   "source": [
    "# Get your own datastore\n",
    "ds2 = Datastore.get(ws, datastore_name='myblob01')\n",
    "\n",
    "# Upload local \"data\" folder (incl. files) as \"tfdata\" folder\n",
    "mnist_dataset2 = Dataset.File.upload_directory(\n",
    "    src_dir='./data',\n",
    "    target=DataPath(ds2, 'tfdata'),\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
